# ğŸ“Š ë¸”ë¼ì¸ë“œ í¬ë¡¤ë§ ë°ì´í„°ì˜ RAG ìµœì í™” ì „ëµ

## ğŸ“‹ ëª©ì°¨
1. [ê¸°ë³¸ RAG êµ¬ì¡° (ìˆ˜ì • ì „)](#ê¸°ë³¸-rag-êµ¬ì¡°-ìˆ˜ì •-ì „)
2. [í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìµœì í™” êµ¬ì¡° (ìˆ˜ì • í›„)](#í•˜ì´ë¸Œë¦¬ë“œ-ê²€ìƒ‰-ìµœì í™”-êµ¬ì¡°-ìˆ˜ì •-í›„)
3. [ì‹¤ì œ ë°ì´í„° í™œìš©ì„ ìœ„í•œ íŠ¹í™” ì „ëµ](#ì‹¤ì œ-ë°ì´í„°-í™œìš©ì„-ìœ„í•œ-íŠ¹í™”-ì „ëµ)
4. [êµ¬í˜„ ê°€ì´ë“œ](#êµ¬í˜„-ê°€ì´ë“œ)

---

## ê¸°ë³¸ RAG êµ¬ì¡° (ìˆ˜ì • ì „)

### 1. ê¸°ë³¸ ë°ì´í„° ë³€í™˜ ì „ëµ

**ì›ë³¸ í¬ë¡¤ë§ ë°ì´í„° â†’ RAG ë¬¸ì„œ ë¶„í• **

```python
# ì›ë³¸ ë°ì´í„° (1í–‰ = 1ë¦¬ë·°)
{
    "ì´ì ": 4.2, "ì›Œë¼ë°¸": 3.5, "ê¸‰ì—¬": 4.5, "ë¬¸í™”": 4.0, "ê²½ì˜ì§„": 3.8,
    "ì œëª©": "ì„±ì¥í•  ìˆ˜ ìˆëŠ” ì¢‹ì€ íšŒì‚¬",
    "ì§ì›ìœ í˜•": "í˜„ì§ì›", 
    "ì¥ì ": "ë³µì§€ê°€ ì¢‹ê³  ì„±ì¥ ê¸°íšŒê°€ ë§ìŒ",
    "ë‹¨ì ": "ì—…ë¬´ ê°•ë„ê°€ ë†’ê³  ì•¼ê·¼ì´ ì¦ìŒ"
}

# RAG ë³€í™˜ í›„ (1ë¦¬ë·° â†’ 3ë¬¸ì„œ)
Document 1: ì¢…í•© í‰ê°€
Document 2: ì¥ì  ì¤‘ì‹¬  
Document 3: ë‹¨ì  ì¤‘ì‹¬
```

### 2. ìì—°ì–´ ì»¨í…ìŠ¤íŠ¸ ê°•í™”

```python
def enrich_rating_context(scores):
    """í‰ì ì„ ìì—°ì–´ë¡œ ë³€í™˜"""
    # 4.2ì  â†’ "ë§¤ìš° ë§Œì¡±ìŠ¤ëŸ¬ìš´ ìˆ˜ì¤€"
    # 3.5ì  â†’ "ë³´í†µ ìˆ˜ì¤€" 
    # 2.8ì  â†’ "ì•„ì‰¬ìš´ ìˆ˜ì¤€"
```

### 3. ë©”íƒ€ë°ì´í„° êµ¬ì¡°

```json
{
    "company": "ë„¤ì´ë²„",
    "aspect": "summary", // summary, pros, cons
    "employee_type": "í˜„ì§ì›",
    "rating_total": 4.2,
    "rating_category": "high", // high(4.0+), medium(3.0-4.0), low(3.0-)
    "content_type": "company_review"
}
```

### 4. ê¸°ë³¸ êµ¬ì¡°ì˜ í•œê³„ì 

âŒ **í‚¤ì›Œë“œ ê²€ìƒ‰ ë¶€ì¡±**
- ë²¡í„° ìœ ì‚¬ë„ì—ë§Œ ì˜ì¡´
- "ì•¼ê·¼", "ë³µì§€" ê°™ì€ ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ì–´ë ¤ì›€
- ìì—°ì–´ ë³€í™˜ìœ¼ë¡œ í•µì‹¬ í‚¤ì›Œë“œ í¬ì„

âŒ **ì „ë¬¸ ê²€ìƒ‰ ë¯¸ì§€ì›**
- BM25 ê°™ì€ ì „í†µì  ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ í™œìš© ë¶ˆê°€
- í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° ë¡œì§ ì—†ìŒ

---

## í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìµœì í™” êµ¬ì¡° (ìˆ˜ì • í›„)

### 1. ë‹¤ì¸µ ë°ì´í„° êµ¬ì¡°

```python
def create_hybrid_optimized_document(row):
    return {
        # ë²¡í„° ê²€ìƒ‰ìš© (ì˜ë¯¸ì  ìœ ì‚¬ë„)
        "content_semantic": f"""
        ë„¤ì´ë²„ì— ëŒ€í•œ í˜„ì§ì› í‰ê°€: ì´ì  4.2ì ìœ¼ë¡œ ë†’ì€ ë§Œì¡±ë„.
        ê¸‰ì—¬ ë³µì§€ 4.5ì ìœ¼ë¡œ ë§Œì¡±ìŠ¤ëŸ¬ìš°ë‚˜, ì›Œë¼ë°¸ 3.5ì ìœ¼ë¡œ ê°œì„  í•„ìš”.
        ì¥ì : {enhanced_pros}
        ë‹¨ì : {enhanced_cons}
        """,
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ìš© (ì›ë³¸ ë³´ì¡´ + ê°•í™”)
        "content_keyword": f"""
        ì œëª©: {row['ì œëª©']}
        ì¥ì : {row['ì¥ì ']} 
        ë‹¨ì : {row['ë‹¨ì ']}
        íšŒì‚¬: ë„¤ì´ë²„
        ì§ì›ìœ í˜•: {row['ì§ì›ìœ í˜•']}
        """,
        
        # ì¶”ì¶œëœ í‚¤ì›Œë“œ ë° ê°€ì¤‘ì¹˜
        "keywords": {
            "extracted": ["ì•¼ê·¼", "ë³µì§€", "ì„±ì¥ê¸°íšŒ"],
            "weighted": {"ì•¼ê·¼": 0.85, "ë³µì§€": 0.92},
            "categories": {
                "worklife": ["ì•¼ê·¼", "ì”ì—…"],
                "benefits": ["ë³µì§€", "ì—°ë´‰"],  
                "growth": ["ì„±ì¥ê¸°íšŒ", "ìŠ¹ì§„"]
            },
            "synonyms": {"ì•¼ê·¼": ["ì”ì—…", "ì˜¤ë²„íƒ€ì„"]}
        },
        
        # ê²€ìƒ‰ ìµœì í™” í•„ë“œ
        "search_fields": {
            "title_text": row['ì œëª©'],
            "pros_text": row['ì¥ì '],
            "cons_text": row['ë‹¨ì '], 
            "all_text": f"{row['ì œëª©']} {row['ì¥ì ']} {row['ë‹¨ì ']}"
        },
        
        "metadata": {
            "company": "ë„¤ì´ë²„",
            "aspect": "pros",
            "employee_type": row['ì§ì›ìœ í˜•'],
            "ratings": {...},
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            "department": extract_department(row),
            "seniority": extract_seniority(row),
            "sentiment_score": calculate_sentiment(row)
        }
    }
```

### 2. í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì²˜ë¦¬ ì‹œìŠ¤í…œ

```python
class KeywordProcessor:
    def __init__(self):
        # ë¸”ë¼ì¸ë“œ ë¦¬ë·° íŠ¹í™” í‚¤ì›Œë“œ ì‚¬ì „
        self.company_keywords = {
            "salary": ["ì—°ë´‰", "ê¸‰ì—¬", "ì›”ê¸‰", "ê¸°ë³¸ê¸‰", "ìƒì—¬ê¸ˆ", "ë³´ë„ˆìŠ¤", "ì¸ì„¼í‹°ë¸Œ"],
            "worklife": ["ì•¼ê·¼", "ì”ì—…", "ì˜¤ë²„íƒ€ì„", "ì›Œë¼ë°¸", "í‡´ê·¼ì‹œê°„", "íœ´ê°€"],
            "culture": ["ë¶„ìœ„ê¸°", "ë¬¸í™”", "ë™ë£Œ", "ìƒì‚¬", "ê°‘ì§ˆ", "íšŒì‹", "ëˆˆì¹˜"],
            "growth": ["ì„±ì¥", "ìŠ¹ì§„", "êµìœ¡", "ê²½ë ¥", "ê¸°íšŒ"],
            "management": ["ê²½ì˜ì§„", "ì„ì›", "ëŒ€í‘œ", "ë¦¬ë”ì‹­"],
            "benefits": ["ë³µì§€", "ë³´í—˜", "ìŠ¤í†¡ì˜µì…˜", "ì£¼ì‹", "ì‹ëŒ€", "êµí†µë¹„"]
        }
        
        self.negative_patterns = [
            "ëˆˆì¹˜", "ê°‘ì§ˆ", "ê°•ì œ", "ìŠ¤íŠ¸ë ˆìŠ¤", "ì•¼ê·¼", "ì‚­ê°", 
            "ë¶ˆë§Œ", "í˜ë“¤", "ë¬¸ì œ", "ì•„ì‰¬ìš´", "ë³„ë¡œ"
        ]
        
        self.positive_patterns = [
            "ë§Œì¡±", "ì¢‹", "í›Œë¥­", "ìµœê³ ", "ì¶”ì²œ", "ì„±ì¥", 
            "ê¸°íšŒ", "ììœ ", "í¸ë¦¬", "ì—¬ìœ "
        ]

    def extract_keywords(self, text):
        """ê³ ë„í™”ëœ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # 1. ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë§¤ì¹­
        for category, words in self.company_keywords.items():
            for word in words:
                if word in text:
                    keywords.append({
                        "keyword": word,
                        "category": category,
                        "frequency": text.count(word),
                        "context": self.extract_context(text, word)
                    })
        
        # 2. ê°ì„± í‚¤ì›Œë“œ íƒœê¹…
        sentiment_keywords = self.extract_sentiment_keywords(text)
        
        # 3. ë¶€ì • í‘œí˜„ ê°ì§€
        negative_contexts = self.detect_negative_contexts(text)
        
        return {
            "keywords": keywords,
            "sentiment_keywords": sentiment_keywords, 
            "negative_contexts": negative_contexts
        }

    def extract_context(self, text, keyword):
        """í‚¤ì›Œë“œ ì£¼ë³€ ë§¥ë½ ì¶”ì¶œ"""
        import re
        
        # í‚¤ì›Œë“œ ì „í›„ 10ê¸€ìì”© ì¶”ì¶œ
        pattern = f".{{0,10}}{re.escape(keyword)}.{{0,10}}"
        matches = re.findall(pattern, text)
        return matches[0] if matches else ""
```

### 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì—”ì§„

```python
class HybridSearchEngine:
    def __init__(self):
        self.vector_db = ChromaDB()  # ë²¡í„° ê²€ìƒ‰
        self.text_index = ElasticsearchIndex()  # ì „ë¬¸ ê²€ìƒ‰
        self.metadata_db = MetadataIndex()  # ë©”íƒ€ë°ì´í„° ê²€ìƒ‰
        
    def search(self, query, top_k=10, search_weights=None):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰"""
        
        # ê¸°ë³¸ ê°€ì¤‘ì¹˜
        if not search_weights:
            search_weights = {"semantic": 0.4, "keyword": 0.4, "metadata": 0.2}
        
        # 1. ì¿¼ë¦¬ ë¶„ì„
        parsed_query = self.parse_query(query)
        
        # 2. ë‹¤ì¤‘ ê²€ìƒ‰ ì‹¤í–‰
        results = {}
        
        # ë²¡í„° ê²€ìƒ‰ (ì˜ë¯¸ì  ìœ ì‚¬ë„)
        if search_weights["semantic"] > 0:
            vector_results = self.vector_search(parsed_query["semantic_query"])
            results["vector"] = vector_results
            
        # í‚¤ì›Œë“œ ê²€ìƒ‰ (BM25)
        if search_weights["keyword"] > 0:
            keyword_results = self.keyword_search(parsed_query["keywords"])
            results["keyword"] = keyword_results
            
        # ë©”íƒ€ë°ì´í„° ê²€ìƒ‰
        if search_weights["metadata"] > 0:
            metadata_results = self.metadata_search(parsed_query["filters"])
            results["metadata"] = metadata_results
        
        # 3. ì ìˆ˜ ê²°í•© ë° ë­í‚¹
        final_results = self.combine_scores(results, search_weights)
        
        return final_results[:top_k]
    
    def parse_query(self, query):
        """ì¿¼ë¦¬ ë¶„ì„ ë° ë¶„í•´"""
        # "ë„¤ì´ë²„ ì•¼ê·¼ ë§ì€ê°€ìš”?" â†’ 
        # {
        #   "company": "ë„¤ì´ë²„",
        #   "keywords": ["ì•¼ê·¼"],
        #   "intent": "worklife", 
        #   "sentiment": "negative",
        #   "semantic_query": query,
        #   "filters": {"company": "ë„¤ì´ë²„", "aspect": "cons"}
        # }
        
        parsed = {
            "semantic_query": query,
            "keywords": [],
            "filters": {},
            "intent": "general"
        }
        
        # íšŒì‚¬ëª… ì¶”ì¶œ
        companies = ["ë„¤ì´ë²„", "ì¹´ì¹´ì˜¤", "ì‚¼ì„±", "LG", "í˜„ëŒ€"]
        for company in companies:
            if company in query:
                parsed["filters"]["company"] = company
                break
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keyword_patterns = {
            "ì•¼ê·¼": ["worklife", "negative"],
            "ì—°ë´‰": ["salary", "neutral"], 
            "ë³µì§€": ["benefits", "positive"],
            "ë¶„ìœ„ê¸°": ["culture", "neutral"]
        }
        
        for keyword, (category, sentiment) in keyword_patterns.items():
            if keyword in query:
                parsed["keywords"].append(keyword)
                parsed["intent"] = category
                if sentiment == "negative":
                    parsed["filters"]["aspect"] = "cons"
                elif sentiment == "positive":
                    parsed["filters"]["aspect"] = "pros"
        
        return parsed
    
    def combine_scores(self, results, weights):
        """ë‹¤ì¤‘ ê²€ìƒ‰ ê²°ê³¼ ì ìˆ˜ ê²°í•©"""
        combined_scores = {}
        
        # ê° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì •ê·œí™”í•˜ê³  ê²°í•©
        for search_type, weight in weights.items():
            if search_type in results and weight > 0:
                for doc_id, score in results[search_type]:
                    if doc_id not in combined_scores:
                        combined_scores[doc_id] = 0
                    combined_scores[doc_id] += weight * score
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_results = sorted(
            combined_scores.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        return sorted_results
```

### 4. ë™ì˜ì–´ ë° ì»¨í…ìŠ¤íŠ¸ í™•ì¥

```python
class SynonymExpander:
    def __init__(self):
        self.synonym_map = {
            # ì›Œë¼ë°¸ ê´€ë ¨
            "ì•¼ê·¼": ["ì”ì—…", "ì˜¤ë²„íƒ€ì„", "ëŠ¦ì€í‡´ê·¼", "ë°¤ìƒ˜"],
            "ì›Œë¼ë°¸": ["ì—…ë¬´ì™€ì‚¶ì˜ê· í˜•", "ì¼ê³¼ì‚¶ì˜ê· í˜•", "ê·¼ë¬´ì‹œê°„"],
            "í‡´ê·¼": ["ì¹¼í‡´", "ì •ì‹œí‡´ê·¼", "ì—…ë¬´ì¢…ë£Œ"],
            
            # ì—°ë´‰ ê´€ë ¨  
            "ì—°ë´‰": ["ê¸‰ì—¬", "ì›”ê¸‰", "ì„ê¸ˆ", "í˜ì´", "ì†Œë“"],
            "ìƒì—¬ê¸ˆ": ["ë³´ë„ˆìŠ¤", "ì¸ì„¼í‹°ë¸Œ", "íŠ¹ë³„ê¸‰ì—¬"],
            "ê¸°ë³¸ê¸‰": ["ë² ì´ìŠ¤", "ê³ ì •ê¸‰", "ê¸°ë³¸ì„ê¸ˆ"],
            
            # ë¬¸í™” ê´€ë ¨
            "ë¶„ìœ„ê¸°": ["ë¬¸í™”", "í™˜ê²½", "ë¶„ìœ„ê¸°"],
            "ê°‘ì§ˆ": ["ê¶Œìœ„ì ", "ê°•ì••ì ", "ì¼ë°©ì "],
            "ëˆˆì¹˜": ["ì•”ë¬µì ê°•ìš”", "ë¶„ìœ„ê¸°ì••ë°•"],
            
            # ì„±ì¥ ê´€ë ¨
            "ì„±ì¥": ["ë°œì „", "í–¥ìƒ", "ì„±ì¥ê¸°íšŒ", "ê²½ë ¥ê°œë°œ"],
            "ìŠ¹ì§„": ["ì§„ê¸‰", "ì»¤ë¦¬ì–´ì—…", "ì§ê¸‰ìƒìŠ¹"]
        }
    
    def expand_query(self, keywords):
        """í‚¤ì›Œë“œë¥¼ ë™ì˜ì–´ë¡œ í™•ì¥"""
        expanded = set(keywords)
        
        for keyword in keywords:
            if keyword in self.synonym_map:
                expanded.update(self.synonym_map[keyword])
        
        return list(expanded)
```

---

## ì‹¤ì œ ë°ì´í„° í™œìš©ì„ ìœ„í•œ íŠ¹í™” ì „ëµ

### 1. ì§„ì§œ ì—°ë´‰ ì •ë³´ ì¶”ì¶œ ë° êµ¬ì¡°í™”

```python
class SalaryIntelligenceExtractor:
    """ì—°ë´‰ ì •ë³´ì˜ ìˆ¨ê²¨ì§„ ì§„ì‹¤ ì¶”ì¶œ"""
    
    def __init__(self):
        self.salary_patterns = {
            "ì‹¤ìˆ˜ë ¹ì•¡": ["ì„¸í›„", "ì‹¤ì œë°›ëŠ”", "ì†ì—ì¥ëŠ”", "ì‹¤ìˆ˜ë ¹", "net"],
            "ê¸°ë³¸ê¸‰ë¹„ìœ¨": ["ê¸°ë³¸ê¸‰", "ë² ì´ìŠ¤", "ê³ ì •ê¸‰", "ë¹„ìœ¨"],
            "ìƒì—¬ê¸ˆì‚­ê°": ["ìƒì—¬ê¸ˆì‚­ê°", "ë³´ë„ˆìŠ¤ì»·", "ì¸ì„¼í‹°ë¸Œì—†ìŒ"],
            "ë¶€ì„œì°¨ì´": ["ë¶€ì„œë³„", "íŒ€ë³„", "ì§ë¬´ë³„", "ì—°ë´‰ì°¨ì´"],
            "ì¸ìƒí˜„ì‹¤": ["ì¸ìƒë¥ ", "ì—°ë´‰í˜‘ìƒ", "ì‹¤ì œì¸ìƒ", "í˜ì´í¬ì¸ìƒ"]
        }
        
        self.negative_salary_indicators = [
            "ë§ë¿ì¸", "ëª…ëª©ìƒ", "ê²‰ìœ¼ë¡œë§Œ", "ì‹¤ì œë¡ ", "ì†ì—¬ì„œ",
            "í˜ì´í¬", "ê°€ì§œ", "ëˆˆì†ì„", "ì‚­ê°", "ì—†ì–´ì§"
        ]
    
    def extract_salary_intelligence(self, review_text):
        """ì—°ë´‰ ê´€ë ¨ í•µì‹¬ ì •ë³´ ì¶”ì¶œ"""
        
        intelligence = {
            "actual_salary_gap": self.detect_actual_vs_nominal(review_text),
            "hidden_structure": self.detect_hidden_salary_structure(review_text),  
            "department_differences": self.detect_department_gaps(review_text),
            "raise_reality": self.detect_raise_reality(review_text)
        }
        
        return intelligence
    
    def detect_actual_vs_nominal(self, text):
        """ëª…ëª© ì—°ë´‰ vs ì‹¤ìˆ˜ë ¹ì•¡ ì°¨ì´ ê°ì§€"""
        patterns = [
            r"(\d+ì²œë§Œì›||\d+ì–µ).*(ì‹¤ì œ|ì„¸í›„|ì†ì—).*(\\d+ì²œë§Œì›|\d+ë°±ë§Œì›)",
            r"(ëª…ëª©|ì•¡ë©´).*(ì‹¤ì œ|ì§„ì§œ).*(ì°¨ì´|ê°­|ë‹¤ë¦„)",
            r"(ë§ë¡œëŠ”|ê³µì‹ì ìœ¼ë¡ ).*(ì‹¤ì œë¡ |ì‚¬ì‹¤ì€)"
        ]
        
        matches = []
        for pattern in patterns:
            import re
            found = re.findall(pattern, text)
            if found:
                matches.extend(found)
        
        return matches
    
    def detect_hidden_salary_structure(self, text):
        """ìˆ¨ê²¨ì§„ ì—°ë´‰ êµ¬ì¡° íŒŒì•…"""
        structure_indicators = {
            "low_base_ratio": ["ê¸°ë³¸ê¸‰ì´ë‚®", "ë² ì´ìŠ¤ì ", "ê³ ì •ê¸‰ë¹„ìœ¨"],
            "bonus_cuts": ["ìƒì—¬ê¸ˆì‚­ê°", "ë³´ë„ˆìŠ¤ì»·", "ì¸ì„¼í‹°ë¸Œì—†ì•°"],
            "fake_allowances": ["ìˆ˜ë‹¹ëª…ëª©", "ê°€ì§œìˆ˜ë‹¹", "í—ˆìš¸ë¿"]
        }
        
        found_structures = {}
        for indicator_type, patterns in structure_indicators.items():
            for pattern in patterns:
                if pattern in text:
                    found_structures[indicator_type] = True
                    break
        
        return found_structures
```

### 2. ì›Œë¼ë°¸ ë¯¼ë‚¯ ë¶„ì„

```python
class WorkLifeRealityExtractor:
    """ì›Œë¼ë°¸ì˜ ìˆ¨ê²¨ì§„ ì§„ì‹¤ ì¶”ì¶œ"""
    
    def __init__(self):
        self.worklife_deception_patterns = {
            "fake_flex_time": ["ì •ì‹œí‡´ê·¼ê¶Œì¥í•˜ì§€ë§Œ", "ëˆˆì¹˜ê²Œì„", "ëª…ëª©ìƒì •ì‹œ"],
            "hidden_overtime": ["ì¹´í†¡ì—…ë¬´", "ì§‘ì—ì„œì¼", "ë³´ì´ì§€ì•ŠëŠ”ì•¼ê·¼"],
            "vacation_pressure": ["ëˆˆì¹˜ë³´ë©°íœ´ê°€", "ììœ ë¡­ë‹¤ì§€ë§Œ", "ì‹¤ì œë¡ ëª»ì¨"],
            "weekend_contact": ["ì£¼ë§ì—°ë½", "íœ´ì¼ì—…ë¬´", "24ì‹œê°„ëŒ€ê¸°"]
        }
    
    def extract_worklife_reality(self, review_text):
        """ì›Œë¼ë°¸ í˜„ì‹¤ ë¶„ì„"""
        
        reality_check = {
            "actual_leaving_time": self.detect_actual_leaving_time(review_text),
            "hidden_overtime_culture": self.detect_hidden_overtime(review_text),
            "vacation_reality": self.detect_vacation_pressure(review_text),
            "weekend_intrusion": self.detect_weekend_work(review_text)
        }
        
        return reality_check
    
    def detect_actual_leaving_time(self, text):
        """ì‹¤ì œ í‡´ê·¼ì‹œê°„ íŒ¨í„´ ë¶„ì„"""
        time_patterns = [
            r"(\d{1,2}ì‹œ).*(í‡´ê·¼|ëë‚¨|ë§ˆê°)",
            r"(ì •ì‹œí‡´ê·¼|ì¹¼í‡´).*(ëˆˆì¹˜|ì–´ë ¤|ë¶ˆê°€ëŠ¥)",
            r"(\d{1,2}ì‹œ).*(ì¼ìƒ|ë³´í†µ|í‰ê· )"
        ]
        
        actual_times = []
        for pattern in time_patterns:
            import re
            matches = re.findall(pattern, text)
            actual_times.extend(matches)
        
        return actual_times
    
    def detect_hidden_overtime(self, text):
        """ìˆ¨ê²¨ì§„ ì•¼ê·¼ ë¬¸í™” ê°ì§€"""
        hidden_patterns = [
            "ì¹´í†¡ìœ¼ë¡œì—…ë¬´", "ì§‘ì—ì„œì¼", "ì¬íƒì´ë¼ì§€ë§Œ", 
            "ë³´ì´ì§€ì•ŠëŠ”ì•¼ê·¼", "ìˆ¨ì€ì—…ë¬´", "ë¹„ê³µì‹ì "
        ]
        
        detected = []
        for pattern in hidden_patterns:
            if pattern in text.replace(" ", ""):
                detected.append(pattern)
        
        return detected
```

### 3. ì¡°ì§ë¬¸í™” & ì¸ê°„ê´€ê³„ ë¦¬ì–¼ ë¶„ì„

```python
class OrganizationCultureAnalyzer:
    """ì¡°ì§ë¬¸í™”ì˜ ìˆ¨ê²¨ì§„ ë©´ ë¶„ì„"""
    
    def __init__(self):
        self.toxic_culture_patterns = {
            "power_abuse": ["ê°‘ì§ˆ", "ê¶Œìœ„ì ", "ì¼ë°©ì ", "ê°•ì••ì "],
            "internal_politics": ["íŒŒë²Œ", "ì¤„ì„œê¸°", "ì •ì¹˜ì ", "í¸ê°€ë¥´ê¸°"],
            "promotion_unfairness": ["ì‹¤ë ¥ë¬´ì‹œ", "í•™ë²Œì°¨ë³„", "ì„±ë³„ì°¨ë³„", "ì¤„ì„œê¸°ìŠ¹ì§„"],
            "forced_socializing": ["ê°•ì œíšŒì‹", "ìˆ ìë¦¬ì••ë°•", "ì°¸ì—¬ê°•ìš”"],
            "harassment": ["ê´´ë¡­í˜", "ì„±í¬ë¡±", "ë”°ëŒë¦¼", "ì¸ê²©ëª¨ë…"]
        }
    
    def analyze_toxic_culture(self, review_text):
        """ë…ì„± ë¬¸í™” ìš”ì†Œ ë¶„ì„"""
        
        toxic_elements = {}
        
        for culture_type, patterns in self.toxic_culture_patterns.items():
            detected_patterns = []
            for pattern in patterns:
                if pattern in review_text:
                    context = self.extract_context(review_text, pattern, 20)
                    detected_patterns.append({
                        "pattern": pattern,
                        "context": context,
                        "severity": self.assess_severity(context)
                    })
            
            if detected_patterns:
                toxic_elements[culture_type] = detected_patterns
        
        return toxic_elements
    
    def detect_specific_person_issues(self, text):
        """íŠ¹ì • ì¸ë¬¼ ê´€ë ¨ ë¬¸ì œ ê°ì§€"""
        person_patterns = [
            r"(íŒ€ì¥|ë¶€ì¥|ì„ì›|ëŒ€í‘œ).*(ê°‘ì§ˆ|ë¬¸ì œ|ì„±ê²©)",
            r"(OOíŒ€ì¥|XXë¶€ì¥).*(í˜ë“ |ë¬¸ì œ)", 
            r"íŠ¹ì •.*(ìƒì‚¬|ì„ì›).*(ë¬¸ì œ|ê°‘ì§ˆ)"
        ]
        
        person_issues = []
        for pattern in person_patterns:
            import re
            matches = re.findall(pattern, text)
            person_issues.extend(matches)
        
        return person_issues
```

### 4. íšŒì‚¬ ë‚´ë¶€ ì‚¬ì • ì¸í…”ë¦¬ì „ìŠ¤

```python
class CompanyIntelligenceAnalyzer:
    """íšŒì‚¬ ë‚´ë¶€ ì‚¬ì • ë¶„ì„"""
    
    def __init__(self):
        self.financial_reality_patterns = {
            "fake_profitability": ["í‘ìë¼ì§€ë§Œ", "ì‹¤ì œë¡ ì ì", "ìˆ«ìì¡°ì‘"],
            "restructuring_signs": ["êµ¬ì¡°ì¡°ì •ì§•ì¡°", "ì¸ë ¥ê°ì¶•", "ë¶€ì„œí†µíí•©"],
            "department_discrimination": ["í™€ëŒ€ë°›ëŠ”ë¶€ì„œ", "ì²œë•ê¾¸ëŸ¬ê¸°", "ì˜ˆì‚°ì‚­ê°"],
            "labor_conflicts": ["ë…¸ì¡°ê°ˆë“±", "íŒŒì—…", "ë…¸ì‚¬ë¶„ê·œ"]
        }
    
    def analyze_internal_intelligence(self, review_text):
        """ë‚´ë¶€ ì •ë³´ ë¶„ì„"""
        
        intelligence = {
            "financial_reality": self.detect_financial_issues(review_text),
            "restructuring_signals": self.detect_restructuring_signs(review_text),
            "department_issues": self.detect_department_discrimination(review_text),
            "labor_relations": self.detect_labor_conflicts(review_text)
        }
        
        return intelligence
    
    def detect_financial_issues(self, text):
        """ì¬ì • ìƒí™© ì§„ì‹¤ ê°ì§€"""
        financial_indicators = [
            "ì‹¤ì œë¡ ì ì", "ìˆ¨ê²¨ì§„ë¶€ì±„", "íšŒê³„ì¡°ì‘", "ìê¸ˆë‚œ",
            "íˆ¬ììœ ì¹˜ì‹¤íŒ¨", "ë§¤ì¶œí•˜ë½", "ì ìì „í™˜"
        ]
        
        detected_issues = []
        for indicator in financial_indicators:
            if indicator in text.replace(" ", ""):
                detected_issues.append(indicator)
        
        return detected_issues
```

### 5. ì—…ë¬´í™˜ê²½ ì§„ì‹¤ ë¶„ì„

```python
class WorkEnvironmentAnalyzer:
    """ì—…ë¬´ í™˜ê²½ì˜ í˜„ì‹¤ ë¶„ì„"""
    
    def __init__(self):
        self.environment_reality_patterns = {
            "workload_intensity": ["ë¬´ë¦¬í•œì—…ë¬´", "ì¸ë ¥ë¶€ì¡±", "ê³¼ë„í•œì—…ë¬´"],
            "equipment_issues": ["ë…¸í›„ì¥ë¹„", "ëŠë¦°ì»´í“¨í„°", "ì‹œì„¤ë‚™í›„"],
            "system_problems": ["ë¶ˆí¸í•œì‹œìŠ¤í…œ", "ITì¸í”„ë¼ë¶€ì¡±", "ì—…ë¬´íš¨ìœ¨ì €í•˜"]
        }
    
    def analyze_work_environment(self, review_text):
        """ì—…ë¬´ í™˜ê²½ í˜„ì‹¤ ë¶„ì„"""
        
        environment_analysis = {
            "actual_workload": self.detect_workload_reality(review_text),
            "equipment_reality": self.detect_equipment_issues(review_text),
            "system_efficiency": self.detect_system_problems(review_text)
        }
        
        return environment_analysis
    
    def detect_workload_reality(self, text):
        """ì‹¤ì œ ì—…ë¬´ ê°•ë„ ê°ì§€"""
        intensity_patterns = [
            r"(\d+ì‹œê°„|\\d+ê°œì›”).*(ì—°ì†|ê³„ì†|ë¬´ë¦¬)",
            r"(ì—¬ìœ ë¡œì›Œë³´ì´ì§€ë§Œ|í¸í•´ë³´ì´ì§€ë§Œ).*(ì‹¤ì œë¡ |ì‚¬ì‹¤ì€)",
            r"(ê²‰ìœ¼ë¡ |í‘œë©´ì ìœ¼ë¡ ).*(ë‚´ë¶€ì ìœ¼ë¡ |ì‹¤ìƒì€)"
        ]
        
        workload_reality = []
        for pattern in intensity_patterns:
            import re
            matches = re.findall(pattern, text)
            workload_reality.extend(matches)
        
        return workload_reality
```

### 6. í†µí•© í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì „ëµ

```python
class AdvancedHybridSearchStrategy:
    """íŠ¹í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì „ëµ"""
    
    def __init__(self):
        self.domain_specific_extractors = {
            "salary": SalaryIntelligenceExtractor(),
            "worklife": WorkLifeRealityExtractor(), 
            "culture": OrganizationCultureAnalyzer(),
            "internal": CompanyIntelligenceAnalyzer(),
            "environment": WorkEnvironmentAnalyzer()
        }
    
    def create_specialized_documents(self, review_data):
        """íŠ¹í™”ëœ ë¬¸ì„œ ìƒì„±"""
        
        specialized_docs = []
        
        for _, row in review_data.iterrows():
            review_text = f"{row['ì œëª©']} {row['ì¥ì ']} {row['ë‹¨ì ']}"
            
            # ê¸°ë³¸ ë¬¸ì„œ ìƒì„±
            base_docs = self.create_base_documents(row)
            
            # ë„ë©”ì¸ë³„ íŠ¹í™” ì •ë³´ ì¶”ì¶œ
            for domain, extractor in self.domain_specific_extractors.items():
                specialized_info = extractor.analyze(review_text)
                
                if specialized_info:  # ìœ ì˜ë¯¸í•œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                    specialized_doc = {
                        "content": self.create_specialized_content(
                            row, domain, specialized_info
                        ),
                        "metadata": {
                            **base_docs[0]["metadata"],
                            "specialization": domain,
                            "intelligence_level": "high",
                            "extracted_insights": specialized_info
                        },
                        "keywords": {
                            **base_docs[0]["keywords"],
                            "specialized_keywords": self.extract_domain_keywords(
                                domain, specialized_info
                            )
                        }
                    }
                    specialized_docs.append(specialized_doc)
        
        return specialized_docs
    
    def search_with_intelligence(self, query, intelligence_focus=None):
        """ì¸í…”ë¦¬ì „ìŠ¤ ì¤‘ì‹¬ ê²€ìƒ‰"""
        
        # ì¿¼ë¦¬ ì˜ë„ ë¶„ì„
        query_intent = self.analyze_query_intent(query)
        
        # ì¸í…”ë¦¬ì „ìŠ¤ í¬ì»¤ìŠ¤ê°€ ìˆëŠ” ê²½ìš° ê°€ì¤‘ì¹˜ ì¡°ì •
        if intelligence_focus:
            search_weights = self.adjust_weights_for_intelligence(
                intelligence_focus, query_intent
            )
        else:
            search_weights = {"semantic": 0.3, "keyword": 0.4, "intelligence": 0.3}
        
        # íŠ¹í™” ê²€ìƒ‰ ì‹¤í–‰
        results = self.hybrid_search_engine.search(
            query, 
            search_weights=search_weights,
            intelligence_filter=intelligence_focus
        )
        
        return results
    
    def analyze_query_intent(self, query):
        """ì¿¼ë¦¬ ì˜ë„ ì‹¬ì¸µ ë¶„ì„"""
        
        intent_patterns = {
            "salary_reality": ["ì‹¤ì œì—°ë´‰", "ì§„ì§œê¸‰ì—¬", "ì„¸í›„ìˆ˜ë ¹", "ì‹¤ìˆ˜ë ¹ì•¡"],
            "worklife_truth": ["ì§„ì§œì›Œë¼ë°¸", "ì‹¤ì œí‡´ê·¼ì‹œê°„", "ì•¼ê·¼í˜„ì‹¤"],
            "culture_dark": ["ê°‘ì§ˆ", "ë…ì„±ë¬¸í™”", "ì¸ê°„ê´€ê³„", "íŒŒë²Œ"],
            "internal_info": ["ë‚´ë¶€ì‚¬ì •", "êµ¬ì¡°ì¡°ì •", "íšŒì‚¬ìƒí™©", "ì¬ì •"],
            "environment_real": ["ì—…ë¬´ê°•ë„", "ì‹œì„¤í˜„í™©", "ì—…ë¬´í™˜ê²½"]
        }
        
        detected_intents = []
        for intent, patterns in intent_patterns.items():
            for pattern in patterns:
                if pattern in query:
                    detected_intents.append(intent)
        
        return detected_intents
    
    def adjust_weights_for_intelligence(self, focus, query_intent):
        """ì¸í…”ë¦¬ì „ìŠ¤ í¬ì»¤ìŠ¤ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì¡°ì •"""
        
        weight_strategies = {
            "salary_reality": {"semantic": 0.2, "keyword": 0.5, "intelligence": 0.3},
            "worklife_truth": {"semantic": 0.3, "keyword": 0.4, "intelligence": 0.3},
            "culture_dark": {"semantic": 0.4, "keyword": 0.3, "intelligence": 0.3},
            "internal_info": {"semantic": 0.2, "keyword": 0.3, "intelligence": 0.5},
            "environment_real": {"semantic": 0.3, "keyword": 0.4, "intelligence": 0.3}
        }
        
        if focus in weight_strategies:
            return weight_strategies[focus]
        else:
            return {"semantic": 0.3, "keyword": 0.4, "intelligence": 0.3}
```

### 7. ì‹¤ì œ í™œìš© ì‹œë‚˜ë¦¬ì˜¤

```python
# ì‹œë‚˜ë¦¬ì˜¤ 1: ì—°ë´‰ í˜„ì‹¤ íŒŒì•…
query = "ë„¤ì´ë²„ ê°œë°œì ì‹¤ì œ ì—°ë´‰ ì–¼ë§ˆë‚˜ ë°›ë‚˜ìš”?"
results = search_engine.search_with_intelligence(
    query, 
    intelligence_focus="salary_reality"
)

# ì‹œë‚˜ë¦¬ì˜¤ 2: ì›Œë¼ë°¸ ì§„ì‹¤ íƒêµ¬  
query = "ì¹´ì¹´ì˜¤ ì •ë§ ì›Œë¼ë°¸ ì¢‹ì€ê°€ìš”?"
results = search_engine.search_with_intelligence(
    query,
    intelligence_focus="worklife_truth" 
)

# ì‹œë‚˜ë¦¬ì˜¤ 3: ì¡°ì§ë¬¸í™” ë¯¼ë‚¯
query = "ì‚¼ì„±ì „ì ê°‘ì§ˆ ë¬¸í™” ì–´ë–¤ê°€ìš”?"
results = search_engine.search_with_intelligence(
    query,
    intelligence_focus="culture_dark"
)
```

---

## êµ¬í˜„ ê°€ì´ë“œ

### 1. ê¸°ìˆ  ìŠ¤íƒ ê¶Œì¥ì‚¬í•­

**ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤**
- **ChromaDB**: ê°œë°œ/í”„ë¡œí† íƒ€ì´í•‘
- **Pinecone**: ëŒ€ìš©ëŸ‰/í”„ë¡œë•ì…˜
- **Weaviate**: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ íŠ¹í™”

**ì „ë¬¸ ê²€ìƒ‰**  
- **Elasticsearch**: ê³ ì„±ëŠ¥ ì „ë¬¸ ê²€ìƒ‰
- **OpenSearch**: ì˜¤í”ˆì†ŒìŠ¤ ëŒ€ì•ˆ
- **Whoosh**: íŒŒì´ì¬ ë„¤ì´í‹°ë¸Œ

**ì„ë² ë”© ëª¨ë¸**
- **í•œêµ­ì–´ íŠ¹í™”**: KoBERT, KoSBERT
- **ë‹¤êµ­ì–´**: multilingual-E5, BGE-M3
- **ê²½ëŸ‰í™”**: sentence-transformers/all-MiniLM-L6-v2

### 2. êµ¬í˜„ ë‹¨ê³„

**Phase 1: ê¸°ë³¸ RAG êµ¬ì¶•**
1. í¬ë¡¤ë§ ë°ì´í„° â†’ ê¸°ë³¸ ë¬¸ì„œ ë³€í™˜
2. ë²¡í„° ì„ë² ë”© ë° ì €ì¥
3. ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê²€ìƒ‰ êµ¬í˜„

**Phase 2: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¶”ê°€**
1. í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œìŠ¤í…œ êµ¬ì¶•
2. ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„±
3. ì ìˆ˜ ê²°í•© ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„

**Phase 3: ì¸í…”ë¦¬ì „ìŠ¤ íŠ¹í™”**
1. ë„ë©”ì¸ë³„ íŠ¹í™” ì¶”ì¶œê¸° ê°œë°œ
2. ê³ ê¸‰ ì¿¼ë¦¬ ë¶„ì„ ì‹œìŠ¤í…œ
3. ê°œì¸í™” ë° ì»¨í…ìŠ¤íŠ¸ ì¸ì‹

### 3. ì„±ëŠ¥ ìµœì í™”

**ì¸ë±ì‹± ìµœì í™”**
```python
# ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì¸ë±ì‹± ì†ë„ í–¥ìƒ
batch_size = 1000
for i in range(0, len(documents), batch_size):
    batch = documents[i:i+batch_size]
    vector_db.add_batch(batch)
    
# ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ì‘ë‹µ ì†ë„ í–¥ìƒ  
import asyncio

async def hybrid_search_async(query):
    vector_task = asyncio.create_task(vector_search(query))
    keyword_task = asyncio.create_task(keyword_search(query))
    metadata_task = asyncio.create_task(metadata_search(query))
    
    results = await asyncio.gather(vector_task, keyword_task, metadata_task)
    return combine_results(results)
```

**ë©”ëª¨ë¦¬ ê´€ë¦¬**
```python
# ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ì²­í‚¹
def process_large_dataset(data, chunk_size=10000):
    for chunk in pd.read_csv(data, chunksize=chunk_size):
        processed_chunk = process_documents(chunk)
        yield processed_chunk

# ìºì‹±ìœ¼ë¡œ ë°˜ë³µ ê²€ìƒ‰ ìµœì í™”
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_vector_search(query_hash):
    return vector_search(query_hash)
```

### 4. í’ˆì§ˆ í‰ê°€ ì§€í‘œ

**ê²€ìƒ‰ ì„±ëŠ¥ ì§€í‘œ**
- **Precision@K**: ìƒìœ„ Kê°œ ê²°ê³¼ ì¤‘ ê´€ë ¨ì„± ìˆëŠ” ë¹„ìœ¨
- **Recall@K**: ì „ì²´ ê´€ë ¨ ë¬¸ì„œ ì¤‘ ìƒìœ„ Kê°œì—ì„œ ì°¾ì€ ë¹„ìœ¨  
- **MRR (Mean Reciprocal Rank)**: í‰ê·  ì—­ìˆœìœ„
- **NDCG (Normalized DCG)**: ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ì„±ëŠ¥

**í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í‰ê°€**
```python
def evaluate_hybrid_search(test_queries, ground_truth):
    metrics = {
        "precision_at_5": [],
        "recall_at_10": [], 
        "mrr": [],
        "hybrid_effectiveness": []
    }
    
    for query, expected_docs in zip(test_queries, ground_truth):
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰
        results = hybrid_search(query, top_k=10)
        
        # ê° ì§€í‘œ ê³„ì‚°
        precision = calculate_precision_at_k(results, expected_docs, k=5)
        recall = calculate_recall_at_k(results, expected_docs, k=10)
        mrr = calculate_mrr(results, expected_docs)
        
        metrics["precision_at_5"].append(precision)
        metrics["recall_at_10"].append(recall)
        metrics["mrr"].append(mrr)
    
    return {
        metric: np.mean(values) 
        for metric, values in metrics.items()
    }
```

ì´ ì „ëµì„ í†µí•´ ë¸”ë¼ì¸ë“œ í¬ë¡¤ë§ ë°ì´í„°ì—ì„œ **ì§„ì§œ ìœ ìš©í•œ ì¸ì‚¬ì´íŠ¸**ë¥¼ ì •í™•í•˜ê³  íš¨ê³¼ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” ê³ ë„í™”ëœ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.