"""
BlindInsight AI - í’ˆì§ˆ í‰ê°€ ì „ë¬¸ ì—ì´ì „íŠ¸

ğŸ¯ ì „ë¬¸ ì˜ì—­:
- AI ì—ì´ì „íŠ¸ ì‘ë‹µì˜ í’ˆì§ˆ í‰ê°€ ë° ê°œì„  ì œì•ˆ
- ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì‘ë‹µ ê°„ì˜ ì í•©ì„± ë¶„ì„
- ì¬ì‹œë„ ì—¬ë¶€ íŒë‹¨ ë° ê°œì„  ë°©í–¥ ì œì‹œ

ğŸ” í•µì‹¬ ê¸°ëŠ¥:
- ì •í™•ì„±, ì™„ì„±ë„, ìœ ìš©ì„±, êµ¬ì²´ì„±, ê· í˜•ì„± í‰ê°€
- JSON í˜•íƒœì˜ êµ¬ì¡°í™”ëœ í‰ê°€ ê²°ê³¼ ì œê³µ
- LLM ê¸°ë°˜ ìˆœìˆ˜ í’ˆì§ˆ íŒë‹¨ (í´ë°± ì—†ìŒ)

ğŸ“Š í‰ê°€ ê¸°ì¤€:
- 9-10ì : excellent (ì¬ì‹œë„ ë¶ˆí•„ìš”)
- 7-8ì : good (ì¬ì‹œë„ ë¶ˆí•„ìš”)  
- 5-6ì : fair (ì¬ì‹œë„ ê³ ë ¤)
- 0-4ì : poor (ì¬ì‹œë„ í•„ìš”)
"""

from typing import Any, Dict, List, Optional
import json
from ..models.user import UserProfile
from .base import BaseAgent, AgentResult, AgentConfig


class QualityEvaluatorAgent(BaseAgent):
    """
    í’ˆì§ˆ í‰ê°€ ì „ë¬¸ ì—ì´ì „íŠ¸
    
    ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:
    - AI ì—ì´ì „íŠ¸ ì‘ë‹µì˜ ì¢…í•©ì  í’ˆì§ˆ í‰ê°€
    - ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì‘ë‹µ ê°„ì˜ ì í•©ì„± ë¶„ì„
    - ê°œì„  ë°©í–¥ ì œì‹œ ë° ì¬ì‹œë„ ì—¬ë¶€ íŒë‹¨
    - ìˆœìˆ˜ LLM ê¸°ë°˜ í‰ê°€ (RAG ê²€ìƒ‰ ì—†ìŒ)
    """
    
    def __init__(self, config: Optional[AgentConfig] = None):
        """í’ˆì§ˆ í‰ê°€ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        super().__init__(name="quality_evaluator_agent", config=config)
        
        # í’ˆì§ˆ í‰ê°€ ì—ì´ì „íŠ¸ëŠ” RAG ê²€ìƒ‰ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        self.target_collections = []
    
    async def execute(
        self, 
        query: str, 
        context: Optional[Dict[str, Any]] = None,
        user_profile: Optional[UserProfile] = None
    ) -> AgentResult:
        """
        í’ˆì§ˆ í‰ê°€ ì‹¤í–‰
        
        Args:
            query: ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸
            context: í‰ê°€ ëŒ€ìƒ ì‘ë‹µ ë° ì—ì´ì „íŠ¸ ì •ë³´
            user_profile: ì‚¬ìš©ì í”„ë¡œí•„ (ì„ íƒì‚¬í•­)
            
        Returns:
            AgentResult: í’ˆì§ˆ í‰ê°€ ê²°ê³¼
        """
        # ì—ëŸ¬ ì²˜ë¦¬ì™€ í•¨ê»˜ ì‹¤í–‰
        return await self._execute_with_error_handling(
            self._evaluate_response_quality,
            query,
            context,
            user_profile
        )
    
    async def execute_as_supervisor_tool(
        self,
        user_question: str,
        supervisor_prompt: Optional[str],
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Supervisor ë„êµ¬ë¡œ ì‹¤í–‰ - BaseAgent ë©”ì„œë“œ ì˜¤ë²„ë¼ì´ë“œ
        í’ˆì§ˆ í‰ê°€ ì „ìš© ì—ì´ì „íŠ¸ëŠ” RAG ê²€ìƒ‰ ì—†ì´ í’ˆì§ˆ í‰ê°€ë§Œ ìˆ˜í–‰
        
        Args:
            user_question: ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸
            supervisor_prompt: Supervisorê°€ ì œê³µí•œ ì‘ì—… ì„¤ëª… (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            context: í‰ê°€ ëŒ€ìƒ ì‘ë‹µ ë° ì—ì´ì „íŠ¸ ì •ë³´
            
        Returns:
            str: JSON í˜•íƒœì˜ í’ˆì§ˆ í‰ê°€ ê²°ê³¼ (supervisorê°€ íŒŒì‹± ê°€ëŠ¥í•œ í˜•íƒœ)
        """
        try:
            print(f"[{self.name}] ===== execute_as_supervisor_tool ì‹œì‘ =====")
            print(f"[{self.name}] ì‚¬ìš©ì ì§ˆë¬¸: {user_question}")
            print(f"[{self.name}] Supervisor í”„ë¡¬í”„íŠ¸ ì¡´ì¬: {bool(supervisor_prompt)}")
            print(f"[{self.name}] ì»¨í…ìŠ¤íŠ¸: {context}")
            
            # RAG ê²€ìƒ‰ ì—†ì´ ë°”ë¡œ í’ˆì§ˆ í‰ê°€ ìˆ˜í–‰
            result = await self._evaluate_response_quality(
                user_question,
                context,
                None
            )
            
            # ê²°ê³¼ í¬ë§·íŒ… - JSON êµ¬ì¡°ë¡œ ë°˜í™˜ (supervisor íŒŒì‹±ìš©)
            if isinstance(result, dict) and 'evaluation' in result:
                evaluation = result['evaluation']
                
                # Supervisorê°€ íŒŒì‹±í•  ìˆ˜ ìˆëŠ” JSON í˜•íƒœë¡œ ë°˜í™˜
                supervisor_result = {
                    "evaluation_type": "quality_assessment",
                    "should_retry": evaluation.get('should_retry', False),
                    "overall_score": evaluation.get('overall_score', 0),
                    "quality_level": evaluation.get('quality_level', 'fair'),
                    "improvement_suggestions": evaluation.get('improvement_suggestions', []),
                    "detailed_scores": {
                        "accuracy_score": evaluation.get('accuracy_score', 0),
                        "completeness_score": evaluation.get('completeness_score', 0),
                        "usefulness_score": evaluation.get('usefulness_score', 0),
                        "specificity_score": evaluation.get('specificity_score', 0),
                        "balance_score": evaluation.get('balance_score', 0)
                    },
                    "original_response": context.get("last_ai_response", "")[:200] + "..." if context and context.get("last_ai_response", "") and len(context.get("last_ai_response", "")) > 200 else context.get("last_ai_response", "") if context else "",
                    "agent_type": context.get("agent_name", "unknown") if context else "unknown",
                    "confidence_score": result.get('confidence_score', 0.9)
                }
                
                # JSON ë¬¸ìì—´ë¡œ ì§ë ¬í™”í•˜ì—¬ ë°˜í™˜
                import json
                json_result = json.dumps(supervisor_result, ensure_ascii=False, indent=2)
                
                print(f"[{self.name}] í’ˆì§ˆ í‰ê°€ ì™„ë£Œ: {evaluation.get('overall_score', 0)}/10 ({evaluation.get('quality_level', 'fair')}), should_retry: {evaluation.get('should_retry', False)}")
                
                return json_result
            
            # ì˜¤ë¥˜ ì¼€ì´ìŠ¤ - JSON í˜•íƒœë¡œ ë°˜í™˜
            elif isinstance(result, dict) and 'error' in result:
                error_result = {
                    "evaluation_type": "quality_assessment",
                    "should_retry": False,
                    "overall_score": 0,
                    "quality_level": "unknown",
                    "improvement_suggestions": [],
                    "error": result['error'],
                    "confidence_score": 0.0
                }
                return json.dumps(error_result, ensure_ascii=False, indent=2)
            
            # ì˜ˆì™¸ ì¼€ì´ìŠ¤ - JSON í˜•íƒœë¡œ ë°˜í™˜
            else:
                fallback_result = {
                    "evaluation_type": "quality_assessment",
                    "should_retry": False,
                    "overall_score": 0,
                    "quality_level": "unknown",
                    "improvement_suggestions": [],
                    "error": f"ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ í˜•íƒœ: {str(result)}",
                    "confidence_score": 0.0
                }
                return json.dumps(fallback_result, ensure_ascii=False, indent=2)
            
        except Exception as e:
            print(f"[{self.name}] í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            
            # ì˜¤ë¥˜ ì‹œì—ë„ JSON í˜•íƒœë¡œ ë°˜í™˜
            error_result = {
                "evaluation_type": "quality_assessment", 
                "should_retry": False,
                "overall_score": 0,
                "quality_level": "error",
                "improvement_suggestions": [],
                "error": f"í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "confidence_score": 0.0
            }
            return json.dumps(error_result, ensure_ascii=False, indent=2)
    
    async def _evaluate_response_quality(
        self,
        user_question: str,
        context: Optional[Dict[str, Any]] = None,
        user_profile: Optional[UserProfile] = None
    ) -> Dict[str, Any]:
        """
        AI ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ë©”ì¸ ë¡œì§
        
        Args:
            user_question: ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸
            context: í‰ê°€ ëŒ€ìƒ ì‘ë‹µ ë° ì—ì´ì „íŠ¸ ì •ë³´
            user_profile: ì‚¬ìš©ì í”„ë¡œí•„ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            
        Returns:
            Dict[str, Any]: í’ˆì§ˆ í‰ê°€ ê²°ê³¼
        """
        if not context:
            return {
                "error": "í‰ê°€í•  ì»¨í…ìŠ¤íŠ¸ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "confidence_score": 0.0
            }
        
        # í‰ê°€ ëŒ€ìƒ ì‘ë‹µê³¼ ì—ì´ì „íŠ¸ ì •ë³´ ì¶”ì¶œ
        response_content = context.get("last_ai_response", "")
        agent_type = context.get("agent_name", "unknown")
        
        if not response_content:
            return {
                "error": "í‰ê°€í•  ì‘ë‹µì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "confidence_score": 0.0
            }
        
        # í’ˆì§ˆ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±
        evaluation_prompt = self._create_evaluation_prompt(
            user_question, 
            response_content, 
            agent_type
        )
        
        try:
            # LLMì„ ì‚¬ìš©í•˜ì—¬ í’ˆì§ˆ í‰ê°€ ìˆ˜í–‰
            evaluation_response = await self.generate_response(
                prompt=evaluation_prompt,
                context_documents=[],  # RAG ê²€ìƒ‰ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                temperature=0.0,  # ì¼ê´€ëœ í‰ê°€ë¥¼ ìœ„í•´ ë‚®ì€ temperature ì‚¬ìš©
                max_tokens=1500
            )
            
            if not evaluation_response:
                return {
                    "error": "í’ˆì§ˆ í‰ê°€ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                    "confidence_score": 0.0
                }
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                evaluation_json = json.loads(evaluation_response.strip())
                
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                required_fields = [
                    "accuracy_score", "completeness_score", "usefulness_score",
                    "specificity_score", "balance_score", "overall_score",
                    "quality_level", "improvement_suggestions", "should_retry"
                ]
                
                for field in required_fields:
                    if field not in evaluation_json:
                        evaluation_json[field] = self._get_default_value(field)
                
                # ì ìˆ˜ ìœ íš¨ì„± ê²€ì¦ (0-10 ë²”ìœ„)
                score_fields = [
                    "accuracy_score", "completeness_score", "usefulness_score",
                    "specificity_score", "balance_score", "overall_score"
                ]
                
                for field in score_fields:
                    score = evaluation_json.get(field, 0)
                    if not isinstance(score, (int, float)) or score < 0 or score > 10:
                        evaluation_json[field] = 5  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
                
                return {
                    "user_question": user_question,
                    "evaluated_response": response_content[:500] + "..." if len(response_content) > 500 else response_content,
                    "agent_type": agent_type,
                    "evaluation": evaluation_json,
                    "analysis_timestamp": context.get("timestamp"),
                    "confidence_score": 0.9  # í’ˆì§ˆ í‰ê°€ëŠ” ë†’ì€ ì‹ ë¢°ë„
                }
                
            except json.JSONDecodeError as je:
                print(f"í’ˆì§ˆ í‰ê°€ JSON íŒŒì‹± ì‹¤íŒ¨: {str(je)}")
                print(f"ì›ë³¸ ì‘ë‹µ: {evaluation_response}")
                
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í‰ê°€ ê²°ê³¼ ë°˜í™˜
                return {
                    "user_question": user_question,
                    "evaluated_response": response_content[:500] + "..." if len(response_content) > 500 else response_content,
                    "agent_type": agent_type,
                    "evaluation": {
                        "accuracy_score": 5,
                        "completeness_score": 5,
                        "usefulness_score": 5,
                        "specificity_score": 5,
                        "balance_score": 5,
                        "overall_score": 5,
                        "quality_level": "fair",
                        "improvement_suggestions": ["JSON í˜•íƒœë¡œ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."],
                        "should_retry": False
                    },
                    "parsing_error": str(je),
                    "confidence_score": 0.3
                }
        
        except Exception as e:
            print(f"í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                "error": f"í’ˆì§ˆ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "confidence_score": 0.0
            }
    
    def _create_evaluation_prompt(
        self, 
        user_question: str, 
        response_content: str, 
        agent_type: str
    ) -> str:
        """í’ˆì§ˆ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        return f"""
ë‹¹ì‹ ì€ AI ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ AI ì—ì´ì „íŠ¸ì˜ ë‹µë³€ì…ë‹ˆë‹¤.
ì´ ë‹µë³€ì˜ í’ˆì§ˆì„ ê°ê´€ì ì´ê³  ê³µì •í•˜ê²Œ í‰ê°€í•´ì£¼ì„¸ìš”.

**ì‚¬ìš©ì ì§ˆë¬¸:** {user_question}

**ì—ì´ì „íŠ¸ ìœ í˜•:** {agent_type}

**AI ë‹µë³€:** {response_content}

ë‹¤ìŒ 5ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ ê°ê° 0-10ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:

1. **ì •í™•ì„± (Accuracy)**: ë‹µë³€ì´ ì‚¬ìš©ì ì§ˆë¬¸ì— ì ì ˆí•˜ê²Œ ëŒ€ë‹µí•˜ëŠ”ê°€?
   - ì§ˆë¬¸ì˜ í•µì‹¬ì„ ì´í•´í•˜ê³  ì˜¬ë°”ë¥¸ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ì§€
   - ì‚¬ì‹¤ê´€ê³„ì˜ ì •í™•ì„± ë° ì˜¤í•´ë¥¼ ë¶ˆëŸ¬ì¼ìœ¼í‚¬ ì†Œì§€ê°€ ì—†ëŠ”ì§€

2. **ì™„ì„±ë„ (Completeness)**: ë‹µë³€ì´ ì¶©ë¶„íˆ ìƒì„¸í•˜ê³  ì™„ì „í•œê°€?
   - ì§ˆë¬¸ì— ëŒ€í•œ ì¶©ë¶„í•œ ì •ë³´ì™€ ì„¤ëª…ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€
   - ì¶”ê°€ë¡œ ì•Œì•„ì•¼ í•  ì¤‘ìš”í•œ ì •ë³´ê°€ ë¹ ì§€ì§€ ì•Šì•˜ëŠ”ì§€

3. **ìœ ìš©ì„± (Usefulness)**: ì‚¬ìš©ìì—ê²Œ ì‹¤ì§ˆì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ”ê°€?
   - ì‹¤ì œ ì˜ì‚¬ê²°ì •ì´ë‚˜ ë¬¸ì œí•´ê²°ì— ë„ì›€ì´ ë˜ëŠ” ë‚´ìš©ì¸ì§€
   - ì‹¤í–‰ ê°€ëŠ¥í•˜ê³  êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ì§€

4. **êµ¬ì²´ì„± (Specificity)**: êµ¬ì²´ì ì¸ ì‚¬ë¡€ë‚˜ ë°ì´í„°ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?
   - ì¼ë°˜ì ì¸ ì„¤ëª…ë³´ë‹¤ëŠ” êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ìˆ˜ì¹˜ê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€
   - ì¶”ìƒì ì´ì§€ ì•Šê³  ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‚´ìš©ì¸ì§€

5. **ê· í˜•ì„± (Balance)**: ì¥ì ê³¼ ë‹¨ì ì´ ê· í˜•ìˆê²Œ ì œì‹œë˜ì—ˆëŠ”ê°€?
   - í¸í–¥ë˜ì§€ ì•Šê³  ê°ê´€ì ì¸ ê´€ì ì„ ìœ ì§€í•˜ëŠ”ì§€
   - ê¸ì •ì /ë¶€ì •ì  ì¸¡ë©´ì„ ì ì ˆíˆ ê³ ë ¤í–ˆëŠ”ì§€

**ì „ì²´ ì ìˆ˜**ëŠ” ìœ„ 5ê°œ ì ìˆ˜ì˜ í‰ê· ìœ¼ë¡œ ê³„ì‚°í•˜ì„¸ìš”.

**í’ˆì§ˆ ë“±ê¸‰**:
- 9-10ì : excellent (ì¬ì‹œë„ ë¶ˆí•„ìš”)
- 7-8ì : good (ì¬ì‹œë„ ë¶ˆí•„ìš”)  
- 5-6ì : fair (ì¬ì‹œë„ ê³ ë ¤)
- 0-4ì : poor (ì¬ì‹œë„ í•„ìš”)

**ê°œì„ ì‚¬í•­**ì—ëŠ” êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í”¼ë“œë°±ì„ í¬í•¨í•´ì£¼ì„¸ìš”.
ë§Œì•½ ë‹µë³€ì´ ì‚¬ìš©ì ì§ˆë¬¸ì˜ í•µì‹¬ì„ ë²—ì–´ë‚˜ê±°ë‚˜ ì¤‘ìš”í•œ ì •ë³´ê°€ ë¶€ì¡±í•˜ë‹¤ë©´, 
ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ë‚˜ ì ‘ê·¼ ë°©ë²•ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

**should_retry**ëŠ” ì „ì²´ ì ìˆ˜ê°€ 6ì  ë¯¸ë§Œì´ê±°ë‚˜ ì§ˆë¬¸ì˜ í•µì‹¬ì„ ë²—ì–´ë‚œ ê²½ìš° trueë¡œ ì„¤ì •í•˜ì„¸ìš”.

**ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”):**
{{
  "accuracy_score": ì ìˆ˜(0-10),
  "completeness_score": ì ìˆ˜(0-10),
  "usefulness_score": ì ìˆ˜(0-10),
  "specificity_score": ì ìˆ˜(0-10),
  "balance_score": ì ìˆ˜(0-10),
  "overall_score": ì „ì²´ì ìˆ˜(0-10),
  "quality_level": "excellent|good|fair|poor",
  "improvement_suggestions": ["êµ¬ì²´ì ì¸ ê°œì„ ì‚¬í•­1", "êµ¬ì²´ì ì¸ ê°œì„ ì‚¬í•­2", "..."],
  "should_retry": true/false
}}
"""
    
    def _get_default_value(self, field: str):
        """í•„ìˆ˜ í•„ë“œì˜ ê¸°ë³¸ê°’ ë°˜í™˜"""
        if field in ["accuracy_score", "completeness_score", "usefulness_score", 
                     "specificity_score", "balance_score", "overall_score"]:
            return 5
        elif field == "quality_level":
            return "fair"
        elif field == "improvement_suggestions":
            return ["í‰ê°€ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."]
        elif field == "should_retry":
            return False
        else:
            return None