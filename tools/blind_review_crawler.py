"""
ì—…ê·¸ë ˆì´ë“œëœ ë¸”ë¼ì¸ë“œ í¬ë¡¤ë§ ë„êµ¬ v2.0 - í•˜ì´ë¸Œë¦¬ë“œ ì²­í¬ ë°©ì‹
ì¥ì /ë‹¨ì  ë³„ë„ ì²­í¬ ìƒì„±ìœ¼ë¡œ ì •ë°€í•œ RAG ìµœì í™”

ì£¼ìš” ì—…ê·¸ë ˆì´ë“œ:
- ì œëª©, ì¥ì , ë‹¨ì ì„ ë…ë¦½ ì²­í¬ë¡œ ë¶„ë¦¬
- ì¹´í…Œê³ ë¦¬ë³„ ì¥ì /ë‹¨ì  ì²­í¬ ìƒì„±  
- ë²¡í„° DB ìµœì í™”ëœ ë©”íƒ€ë°ì´í„° êµ¬ì¡°
- kss ê¸°ë°˜ ì •í™•í•œ ë¬¸ì¥ ë¶„í• 
"""

#blind_review_crawler.py

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from bs4 import BeautifulSoup
import time
import pandas as pd
import json
import re
import logging
import os
import glob
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import asdict

# ìƒˆë¡œìš´ ëª¨ë“ˆ import
from category_processor import CategorySpecificProcessor, VectorDBOptimizer
from keyword_dictionary import korean_keywords

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('blind_crawler_hybrid_v2.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class HybridBlindReviewCrawler:
    """í•˜ì´ë¸Œë¦¬ë“œ ì²­í¬ ë°©ì‹ ë¸”ë¼ì¸ë“œ ë¦¬ë·° í¬ë¡¤ëŸ¬"""
    
    def __init__(self, headless=False, wait_timeout=10, output_dir="./data/hybrid_vectordb"):
        self.wait_timeout = wait_timeout
        self.driver = None
        self.wait = None
        self.output_dir = output_dir
        self.is_logged_in = False
        
        # í•˜ì´ë¸Œë¦¬ë“œ í”„ë¡œì„¸ì„œë“¤ ì´ˆê¸°í™”
        self.category_processor = CategorySpecificProcessor()
        self.vectordb_optimizer = VectorDBOptimizer()
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # ì²˜ë¦¬ ê²°ê³¼ ì¶”ì 
        self.processing_results = {
            "total_reviews_processed": 0,
            "total_chunks_created": 0,
            "chunk_breakdown": {
                "title_chunks": 0,
                "pros_chunks": 0, 
                "cons_chunks": 0
            },
            "category_breakdown": {
                "career_growth": 0,
                "salary_benefits": 0,
                "work_life_balance": 0,
                "company_culture": 0,
                "management": 0
            },
            "failed_reviews": 0
        }
        
        # Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™”
        self._initialize_driver(headless)
    
    def _initialize_driver(self, headless):
        """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        chrome_options = Options()
        if headless:
            chrome_options.add_argument("--headless")
        
        # ê¸°ë³¸ í•„ìˆ˜ ì„¤ì •
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--disable-features=VizDisplayCompositor")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # User-Agent ì„¤ì •
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36")
        chrome_options.add_argument("--window-size=1920,1080")
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            
            # ìë™í™” íƒì§€ ìš°íšŒ ìŠ¤í¬ë¦½íŠ¸
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined,
                    });
                '''
            })
            
            self.wait = WebDriverWait(self.driver, self.wait_timeout)
            logger.info("Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ (í•˜ì´ë¸Œë¦¬ë“œ v2.0)")
            
        except Exception as e:
            logger.error(f"ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def login_wait(self, url):
        """ë¡œê·¸ì¸ ëŒ€ê¸° ë° ì²˜ë¦¬"""
        try:
            logger.info(f"í˜ì´ì§€ ì ‘ì† ì¤‘: {url}")
            self.driver.get(url)
            time.sleep(5)
            
            # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­ ì‹œë„
            try:
                login_btn = self.wait.until(
                    EC.element_to_be_clickable((By.CLASS_NAME, "btn_signin"))
                )
                self.driver.execute_script("arguments[0].click();", login_btn)
                logger.info("ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")
            
            time.sleep(3)
            
            print("\n" + "="*60)
            print("ğŸ” ë¸”ë¼ì¸ë“œ ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤")
            print("1. ë¸”ë¼ì¸ë“œ ì•±ì—ì„œ ë¡œê·¸ì¸í•˜ì„¸ìš”")
            print("2. 'ë”ë³´ê¸°' â†’ 'ë¸”ë¼ì¸ë“œ ì›¹ ë¡œê·¸ì¸' í´ë¦­") 
            print("3. ì¸ì¦ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
            print("4. ë¡œê·¸ì¸ ì™„ë£Œ í›„ ì•„ë¬´ í‚¤ë‚˜ ëˆŒëŸ¬ì£¼ì„¸ìš”...")
            print("="*60)
            
            input("ë¡œê·¸ì¸ ì™„ë£Œ í›„ Enter í‚¤ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”: ")
            self.is_logged_in = True
            logger.info("ì‚¬ìš©ì ë¡œê·¸ì¸ ì™„ë£Œ í™•ì¸")
            
        except Exception as e:
            logger.error(f"ë¡œê·¸ì¸ ê³¼ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def extract_review_data(self, element):
        """ê°œë³„ ë¦¬ë·°ì—ì„œ ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        try:
            # í‰ì  ì •ë³´ ì¶”ì¶œ
            rating_element = element.find_element(By.CLASS_NAME, "rating")
            
            # ìƒì„¸ í‰ì  ë³´ê¸° í´ë¦­
            try:
                more_rating_btn = rating_element.find_element(By.CLASS_NAME, "more_rating")
                more_rating_btn.click()
                time.sleep(0.5)
            except NoSuchElementException:
                logger.warning("ìƒì„¸ í‰ì  ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ì´ì  ì¶”ì¶œ
            try:
                score_element = rating_element.find_element(By.CLASS_NAME, "num")
                total_score = float(score_element.text.split("\n")[1])
            except (ValueError, IndexError):
                total_score = 0.0
            
            # ìƒì„¸ ì ìˆ˜ ì¶”ì¶œ
            detail_scores = ["0"] * 5
            try:
                detail_elements = element.find_elements(
                    By.CSS_SELECTOR, 
                    "div.review_item_inr > div.rating > div.more_rating > span > span > span > div.ly_rating > div.rating_wp > span.desc > i.blind"
                )
                for i, detail_elem in enumerate(detail_elements[:5]):
                    detail_scores[i] = detail_elem.text
            except Exception as e:
                logger.warning(f"ìƒì„¸ ì ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ì œëª© ì¶”ì¶œ
            try:
                title_element = element.find_element(By.CSS_SELECTOR, "div.review_item_inr > h3.rvtit > a")
                title = self.clean_text(title_element.text)
            except NoSuchElementException:
                title = "ì œëª© ì—†ìŒ"
            
            # ì§ì› ìœ í˜• ì¶”ì¶œ
            try:
                status_element = element.find_element(By.CSS_SELECTOR, "div.review_item_inr > div.auth")
                status_text = status_element.text.split("\n")
                status = status_text[1] if len(status_text) > 1 else "ì •ë³´ ì—†ìŒ"
            except (NoSuchElementException, IndexError):
                status = "ì •ë³´ ì—†ìŒ"
            
            # ì¥ì /ë‹¨ì  ì¶”ì¶œ
            pros = "ì •ë³´ ì—†ìŒ"
            cons = "ì •ë³´ ì—†ìŒ"
            
            try:
                parag_element = element.find_element(By.CSS_SELECTOR, ".parag")
                full_text = parag_element.text
                
                lines = full_text.split('\n')
                
                pros_started = False
                cons_started = False
                pros_lines = []
                cons_lines = []
                
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue
                        
                    if 'ì¥ì ' in line and len(line) < 10:
                        pros_started = True
                        cons_started = False
                        continue
                    elif 'ë‹¨ì ' in line and len(line) < 10:
                        cons_started = True
                        pros_started = False
                        continue
                    elif any(keyword in line for keyword in ['ì¶”ì²œ', 'í‰ì ', 'íšŒì‚¬ì—ê²Œ', 'ê²½ì˜ì§„ì—ê²Œ']):
                        pros_started = False
                        cons_started = False
                        continue
                    
                    if pros_started and line:
                        pros_lines.append(line)
                    elif cons_started and line:
                        cons_lines.append(line)
                
                if pros_lines:
                    pros = self.clean_text(' '.join(pros_lines))
                if cons_lines:
                    cons = self.clean_text(' '.join(cons_lines))
                    
            except Exception as e:
                logger.warning(f"ì¥ì /ë‹¨ì  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            return [
                total_score,
                detail_scores[0],
                detail_scores[1], 
                detail_scores[2],
                detail_scores[3],
                detail_scores[4],
                title,
                status,
                pros,
                cons
            ]
            
        except Exception as e:
            logger.error(f"ë¦¬ë·° ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return [0.0, "0", "0", "0", "0", "0", "ì˜¤ë¥˜", "ì˜¤ë¥˜", "ì¶”ì¶œ ì‹¤íŒ¨", "ì¶”ì¶œ ì‹¤íŒ¨"]
    
    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜"""
        if not text:
            return ""
        
        pattern = '([ã„±-ã…ã…-ã…£]+)'
        text = re.sub(pattern=pattern, repl='', string=text)
        
        pattern = '<[^>]*>'
        text = re.sub(pattern=pattern, repl='', string=text)
        
        pattern = '[^\w\sê°€-í£]'
        text = re.sub(pattern=pattern, repl=' ', string=text)
        
        text = text.replace('\r', '').replace('\n', ' ')
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def process_single_review(self, raw_review: List, company_name: str, review_index: int) -> Dict[str, List]:
        """ë‹¨ì¼ ë¦¬ë·°ë¥¼ í•˜ì´ë¸Œë¦¬ë“œ ì²­í¬ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬"""
        
        # ë¦¬ë·° ë°ì´í„° êµ¬ì¡°í™”
        review_data = {
            "íšŒì‚¬": company_name,
            "ì´ì ": raw_review[0],
            "ì»¤ë¦¬ì–´í–¥ìƒ": raw_review[1], 
            "ì›Œë¼ë°¸": raw_review[2],
            "ê¸‰ì—¬ë³µì§€": raw_review[3],
            "ì‚¬ë‚´ë¬¸í™”": raw_review[4],
            "ê²½ì˜ì§„": raw_review[5],
            "ì œëª©": raw_review[6],
            "ì§ì›ìœ í˜•_ì›ë³¸": raw_review[7],
            "ì¥ì ": raw_review[8],
            "ë‹¨ì ": raw_review[9],
            "id": f"review_{company_name}_{review_index:04d}"
        }
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì²­í¬ ì²˜ë¦¬
        try:
            all_chunks = self.category_processor.process_review_by_categories(review_data)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.processing_results["total_reviews_processed"] += 1
            
            # ì²­í¬ ìœ í˜•ë³„ í†µê³„ ì—…ë°ì´íŠ¸
            self.processing_results["chunk_breakdown"]["title_chunks"] += len(all_chunks.get("title", []))
            self.processing_results["chunk_breakdown"]["pros_chunks"] += len(all_chunks.get("pros", []))
            self.processing_results["chunk_breakdown"]["cons_chunks"] += len(all_chunks.get("cons", []))
            
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ì—…ë°ì´íŠ¸
            for chunk in all_chunks.get("pros", []) + all_chunks.get("cons", []):
                category = chunk.category
                if category in self.processing_results["category_breakdown"]:
                    self.processing_results["category_breakdown"][category] += 1
            
            total_chunks = len(all_chunks.get("title", [])) + len(all_chunks.get("pros", [])) + len(all_chunks.get("cons", []))
            self.processing_results["total_chunks_created"] += total_chunks
            
            return all_chunks
            
        except Exception as e:
            logger.error(f"ë¦¬ë·° {review_index} í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.processing_results["failed_reviews"] += 1
            return {"title": [], "pros": [], "cons": []}
    
    def crawl_company_reviews(self, company_code: str, pages: int = 25):
        """íšŒì‚¬ ë¦¬ë·° í¬ë¡¤ë§ ë° í•˜ì´ë¸Œë¦¬ë“œ ì²­í¬ ë¶„ì„"""
        
        logger.info(f"ğŸš€ {company_code} í¬ë¡¤ë§ ì‹œì‘ (í•˜ì´ë¸Œë¦¬ë“œ v2.0)")
        
        base_url = f"https://www.teamblind.com/kr/company/{company_code}/reviews"
        
        if not self.is_logged_in:
            self.login_wait(base_url)
        
        # ì „ì²´ ë¦¬ë·° ìˆ˜ì§‘
        all_reviews = []
        
        for page in range(1, pages + 1):
            try:
                target_url = f"{base_url}?page={page}"
                logger.info(f"ğŸ“„ í˜ì´ì§€ {page}/{pages} ì²˜ë¦¬ ì¤‘")
                
                self.driver.get(target_url)
                time.sleep(3)
                
                review_elements = self.driver.find_elements(By.CLASS_NAME, "review_item")
                
                if not review_elements:
                    logger.warning(f"í˜ì´ì§€ {page}ì—ì„œ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue
                
                logger.info(f"í˜ì´ì§€ {page}ì—ì„œ {len(review_elements)}ê°œ ë¦¬ë·° ë°œê²¬")
                
                for idx, element in enumerate(review_elements):
                    try:
                        review_data = self.extract_review_data(element)
                        all_reviews.append(review_data)
                        print(f"[{page}-{idx+1}] {review_data[6]} | ì´ì : {review_data[0]}")
                    except Exception as e:
                        logger.error(f"í˜ì´ì§€ {page}, ë¦¬ë·° {idx+1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        continue
                
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        if not all_reviews:
            logger.warning("ì¶”ì¶œëœ ë¦¬ë·° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        logger.info(f"ğŸ“Š ì´ {len(all_reviews)}ê°œ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ. í•˜ì´ë¸Œë¦¬ë“œ ì²­í¬ ë¶„ì„ ì‹œì‘...")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì²­í¬ ë¶„ì„ ë° ì €ì¥
        return self._process_and_save_hybrid_chunks(company_code, all_reviews)
    
    def _process_and_save_hybrid_chunks(self, company_code: str, all_reviews: List) -> bool:
        """í•˜ì´ë¸Œë¦¬ë“œ ì²­í¬ ì²˜ë¦¬ ë° ì €ì¥"""
        
        # ëª¨ë“  ì²­í¬ë¥¼ ìˆ˜ì§‘í•  ë¦¬ìŠ¤íŠ¸
        all_processed_chunks = {
            "title": [],
            "pros": [],
            "cons": []
        }
        
        # ê° ë¦¬ë·°ë¥¼ í•˜ì´ë¸Œë¦¬ë“œ ì²­í¬ë¡œ ì²˜ë¦¬
        total_reviews = len(all_reviews)
        
        for idx, raw_review in enumerate(all_reviews):
            try:
                chunk_groups = self.process_single_review(raw_review, company_code, idx)
                
                # ì²­í¬ ê·¸ë£¹ë³„ë¡œ ìˆ˜ì§‘
                for chunk_type, chunks in chunk_groups.items():
                    all_processed_chunks[chunk_type].extend(chunks)
                
                # ì§„í–‰ìƒí™© ì¶œë ¥
                if (idx + 1) % 10 == 0:
                    progress = (idx + 1) / total_reviews * 100
                    logger.info(f"âš¡ í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì§„í–‰ë¥ : {progress:.1f}% ({idx + 1}/{total_reviews})")
                    
            except Exception as e:
                logger.error(f"ë¦¬ë·° {idx} í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ì²­í¬ ë¶„ì„ ì™„ë£Œ. ë²¡í„° DB ìµœì í™” ì‹œì‘...")
        
        # ë²¡í„° DB ìµœì í™” ë° ì €ì¥
        optimized_chunks = self.vectordb_optimizer.optimize_chunks_for_vectordb(all_processed_chunks)
        
        # íŒŒì¼ ì €ì¥
        file_path = self.vectordb_optimizer.save_vectordb_file(
            company_code, optimized_chunks, self.output_dir
        )
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        self._print_hybrid_processing_summary(company_code, file_path)
        
        return True
    
    def _print_hybrid_processing_summary(self, company_code: str, file_path: str):
        """í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        
        print(f"\n{'='*80}")
        print(f"ğŸ‰ {company_code} í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ë§ ë° ë¶„ì„ ì™„ë£Œ")
        print(f"{'='*80}")
        
        # ì²˜ë¦¬ í†µê³„
        stats = self.processing_results
        total_chunks = stats["total_chunks_created"]
        
        print(f"ğŸ“Š ì²˜ë¦¬ í†µê³„:")
        print(f"  - ì²˜ë¦¬ëœ ë¦¬ë·°: {stats['total_reviews_processed']}ê°œ")
        print(f"  - ì‹¤íŒ¨í•œ ë¦¬ë·°: {stats['failed_reviews']}ê°œ")
        print(f"  - ìƒì„±ëœ ì´ ì²­í¬: {total_chunks}ê°œ")
        
        # ì²­í¬ ìœ í˜•ë³„ í†µê³„
        chunk_breakdown = stats["chunk_breakdown"]
        print(f"\nğŸ·ï¸ ì²­í¬ ìœ í˜•ë³„ í†µê³„:")
        print(f"  - ì œëª© ì²­í¬: {chunk_breakdown['title_chunks']}ê°œ")
        print(f"  - ì¥ì  ì²­í¬: {chunk_breakdown['pros_chunks']}ê°œ")
        print(f"  - ë‹¨ì  ì²­í¬: {chunk_breakdown['cons_chunks']}ê°œ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_breakdown = stats["category_breakdown"]
        print(f"\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì²­í¬ ìˆ˜:")
        category_names_kr = {
            "career_growth": "ì»¤ë¦¬ì–´ í–¥ìƒ",
            "salary_benefits": "ê¸‰ì—¬ ë° ë³µì§€", 
            "work_life_balance": "ì—…ë¬´ì™€ ì‚¶ì˜ ê· í˜•",
            "company_culture": "ì‚¬ë‚´ ë¬¸í™”",
            "management": "ê²½ì˜ì§„"
        }
        
        for category, count in category_breakdown.items():
            category_kr = category_names_kr.get(category, category)
            print(f"  - {category_kr}: {count}ê°œ")
        
        # ìƒì„±ëœ íŒŒì¼
        print(f"\nğŸ“ ìƒì„±ëœ ë²¡í„° DB íŒŒì¼:")
        print(f"  - {os.path.basename(file_path)}")
        
        # í‰ê·  í†µê³„
        avg_chunks_per_review = total_chunks / stats["total_reviews_processed"] if stats["total_reviews_processed"] > 0 else 0
        print(f"\nâ­ í†µê³„ ìš”ì•½:")
        print(f"  - ë¦¬ë·°ë‹¹ í‰ê·  ì²­í¬: {avg_chunks_per_review:.1f}ê°œ")
        print(f"  - ì²˜ë¦¬ ì„±ê³µë¥ : {(stats['total_reviews_processed'] / (stats['total_reviews_processed'] + stats['failed_reviews']) * 100):.1f}%")
        
        print(f"\nğŸ’¡ í•˜ì´ë¸Œë¦¬ë“œ ì²­í¬ ë°©ì‹ìœ¼ë¡œ ì¥ì /ë‹¨ì ì´ ëª…í™•íˆ ë¶„ë¦¬ë˜ì–´")
        print(f"   AI Agentê°€ ë” ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
        print(f"{'='*80}\n")
    
    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            logger.info("ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ")


def run_single_company_hybrid_crawl(company_code: str, pages: int = 25, headless: bool = False):
    """ë‹¨ì¼ ê¸°ì—… í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ë§ ì‹¤í–‰"""
    
    print(f"\në¸”ë¼ì¸ë“œ í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ëŸ¬ v2.0")
    print(f"ëŒ€ìƒ ê¸°ì—…: {company_code}")
    print(f"í¬ë¡¤ë§ í˜ì´ì§€: {pages}")
    print(f"ëª¨ë“œ: {'í—¤ë“œë¦¬ìŠ¤' if headless else 'ì¼ë°˜'}")
    print(f"ì²­í¬ ë°©ì‹: ì¥ì /ë‹¨ì  ë¶„ë¦¬ í•˜ì´ë¸Œë¦¬ë“œ")
    
    crawler = HybridBlindReviewCrawler(
        headless=headless,
        output_dir="./data/hybrid_vectordb"
    )
    
    try:
        success = crawler.crawl_company_reviews(company_code, pages)
        
        if success:
            print(f"\n{company_code} í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {crawler.output_dir}")
            print(f"ì¥ì /ë‹¨ì ì´ ë¶„ë¦¬ëœ ì •ë°€ ì²­í¬ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"{company_code} í¬ë¡¤ë§ ì‹¤íŒ¨")
            
        return success
        
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ í¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return False
    finally:
        crawler.close()


# ê¸°ì¡´ ì½”ë“œì— ì¶”ê°€í•  í•¨ìˆ˜ë“¤

def run_multiple_companies_hybrid_crawl(company_list: List[str], pages: int = 25, headless: bool = False, delay_between_companies: int = 30):
    """ì—¬ëŸ¬ ê¸°ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ë§ ì‹¤í–‰"""
    
    print(f"\në¸”ë¼ì¸ë“œ ë‹¤ì¤‘ ê¸°ì—… í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ëŸ¬ v2.0")
    print(f"ëŒ€ìƒ ê¸°ì—…: {len(company_list)}ê°œ")
    print(f"í¬ë¡¤ë§ í˜ì´ì§€: {pages}")
    print(f"ëª¨ë“œ: {'í—¤ë“œë¦¬ìŠ¤' if headless else 'ì¼ë°˜'}")
    print(f"ê¸°ì—…ê°„ ëŒ€ê¸°ì‹œê°„: {delay_between_companies}ì´ˆ")
    print(f"ì²­í¬ ë°©ì‹: ì¥ì /ë‹¨ì  ë¶„ë¦¬ í•˜ì´ë¸Œë¦¬ë“œ")
    
    # í¬ë¡¤ë§ ê²°ê³¼ ì¶”ì 
    crawling_results = {
        "success": [],
        "failed": [],
        "total_companies": len(company_list),
        "start_time": datetime.now()
    }
    
    # ë‹¨ì¼ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë¡œê·¸ì¸ í•œë²ˆë§Œ í•˜ê¸° ìœ„í•¨)
    crawler = HybridBlindReviewCrawler(
        headless=headless,
        output_dir="./data/hybrid_vectordb"
    )
    
    try:
        print(f"\n{'='*80}")
        print(f"ğŸš€ ë‹¤ì¤‘ ê¸°ì—… í¬ë¡¤ë§ ì‹œì‘ - ì´ {len(company_list)}ê°œ ê¸°ì—…")
        print(f"{'='*80}")
        
        for idx, company_code in enumerate(company_list):
            try:
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"\n[{idx+1}/{len(company_list)}] {current_time} - {company_code} í¬ë¡¤ë§ ì‹œì‘")
                
                # ê°œë³„ í¬ë¡¤ë§ í†µê³„ ì´ˆê¸°í™”
                crawler.processing_results = {
                    "total_reviews_processed": 0,
                    "total_chunks_created": 0,
                    "chunk_breakdown": {
                        "title_chunks": 0,
                        "pros_chunks": 0, 
                        "cons_chunks": 0
                    },
                    "category_breakdown": {
                        "career_growth": 0,
                        "salary_benefits": 0,
                        "work_life_balance": 0,
                        "company_culture": 0,
                        "management": 0
                    },
                    "failed_reviews": 0
                }
                
                success = crawler.crawl_company_reviews(company_code, pages)
                
                if success:
                    crawling_results["success"].append(company_code)
                    print(f"âœ… {company_code} í¬ë¡¤ë§ ì™„ë£Œ")
                else:
                    crawling_results["failed"].append(company_code)
                    print(f"âŒ {company_code} í¬ë¡¤ë§ ì‹¤íŒ¨")
                
                # ë§ˆì§€ë§‰ ê¸°ì—…ì´ ì•„ë‹ˆë¼ë©´ ëŒ€ê¸°
                if idx < len(company_list) - 1:
                    print(f"â±ï¸ ë‹¤ìŒ ê¸°ì—… í¬ë¡¤ë§ê¹Œì§€ {delay_between_companies}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(delay_between_companies)
                    
            except Exception as e:
                logger.error(f"{company_code} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                crawling_results["failed"].append(company_code)
                print(f"âŒ {company_code} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                
                # ì˜¤ë¥˜ ë°œìƒì‹œì—ë„ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                if idx < len(company_list) - 1:
                    print(f"â±ï¸ ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì¸í•œ ë³µêµ¬ ëŒ€ê¸°: {delay_between_companies}ì´ˆ")
                    time.sleep(delay_between_companies)
        
        # ì „ì²´ í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½
        _print_multiple_crawling_summary(crawling_results)
        
        return crawling_results
        
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ë‹¤ì¤‘ í¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        _print_multiple_crawling_summary(crawling_results)
        return crawling_results
    except Exception as e:
        logger.error(f"ë‹¤ì¤‘ í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return crawling_results
    finally:
        crawler.close()


def _print_multiple_crawling_summary(results: Dict):
    """ë‹¤ì¤‘ í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    
    end_time = datetime.now()
    total_time = end_time - results["start_time"]
    
    print(f"\n{'='*80}")
    print(f"ğŸ ë‹¤ì¤‘ ê¸°ì—… í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ë§ ì™„ë£Œ")
    print(f"{'='*80}")
    
    print(f"ğŸ“Š ì „ì²´ í¬ë¡¤ë§ ê²°ê³¼:")
    print(f"  - ì´ ëŒ€ìƒ ê¸°ì—…: {results['total_companies']}ê°œ")
    print(f"  - ì„±ê³µí•œ ê¸°ì—…: {len(results['success'])}ê°œ")
    print(f"  - ì‹¤íŒ¨í•œ ê¸°ì—…: {len(results['failed'])}ê°œ")
    print(f"  - ì„±ê³µë¥ : {len(results['success'])/results['total_companies']*100:.1f}%")
    print(f"  - ì´ ì†Œìš”ì‹œê°„: {total_time}")
    
    if results["success"]:
        print(f"\nâœ… ì„±ê³µí•œ ê¸°ì—… ({len(results['success'])}ê°œ):")
        for i, company in enumerate(results["success"]):
            print(f"  {i+1:2d}. {company}")
    
    if results["failed"]:
        print(f"\nâŒ ì‹¤íŒ¨í•œ ê¸°ì—… ({len(results['failed'])}ê°œ):")
        for i, company in enumerate(results["failed"]):
            print(f"  {i+1:2d}. {company}")
        
        print(f"\nğŸ’¡ ì‹¤íŒ¨í•œ ê¸°ì—…ë“¤ì€ ë‹¤ìŒê³¼ ê°™ì€ ì´ìœ ë¡œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print(f"  - í•´ë‹¹ ê¸°ì—…ì˜ ë¦¬ë·°ê°€ ì—†ê±°ë‚˜ ì ‘ê·¼ ë¶ˆê°€")
        print(f"  - ê¸°ì—… ì½”ë“œê°€ ë¸”ë¼ì¸ë“œì—ì„œ ë‹¤ë¥¸ í˜•íƒœë¡œ ì‚¬ìš©ë¨")
        print(f"  - ì¼ì‹œì ì¸ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜")
        print(f"  - ë¸”ë¼ì¸ë“œ ì„œë²„ ì‘ë‹µ ì§€ì—°")
    
    print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
    print(f"  - ./data/hybrid_vectordb/ ë””ë ‰í† ë¦¬ì— ê° ê¸°ì—…ë³„ ë²¡í„°DB íŒŒì¼ ì €ì¥")
    print(f"  - íŒŒì¼ëª… í˜•ì‹: {datetime.now().strftime('%Y%m%d')}_[ê¸°ì—…ëª…]_hybrid_vectordb.json")
    
    print(f"\nğŸ¯ ëª¨ë“  ê¸°ì—…ì˜ í•˜ì´ë¸Œë¦¬ë“œ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"{'='*80}\n")


def run_top50_korean_companies_crawl(pages: int = 25, headless: bool = False):
    """í•œêµ­ ìƒìœ„ 50ê°œ ê¸°ì—… í¬ë¡¤ë§ (ì‹œê°€ì´ì•¡ ê¸°ì¤€)"""
    
    top50_companies = [
        "ì‚¼ì„±ì „ì", "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "SKí•˜ì´ë‹‰ìŠ¤", "ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤", "NAVER",
        "LGí™”í•™", "í˜„ëŒ€ì°¨", "ì‚¼ì„±SDI", "ì¹´ì¹´ì˜¤", "ê¸°ì•„",
        "POSCOí™€ë”©ìŠ¤", "KBê¸ˆìœµ", "SKì´ë…¸ë² ì´ì…˜", "ì…€íŠ¸ë¦¬ì˜¨", "ì‚¼ì„±ë¬¼ì‚°",
        "ì‹ í•œì§€ì£¼", "í˜„ëŒ€ëª¨ë¹„ìŠ¤", "ì¹´ì¹´ì˜¤ë±…í¬", "SK", "LGì „ì",
        "í•œêµ­ì „ë ¥", "S-Oil", "í•˜ë‚˜ê¸ˆìœµì§€ì£¼", "í¬ë˜í”„í†¤", "ì‚¼ì„±ìƒëª…",
        "LG", "SKí…”ë ˆì½¤", "KT&G", "ì¹´ì¹´ì˜¤í˜ì´", "ì‚¼ì„±ì „ê¸°",
        "ì‚¼ì„±ì—ìŠ¤ë””ì—ìŠ¤", "ìš°ë¦¬ê¸ˆìœµì§€ì£¼", "ê³ ë ¤ì•„ì—°", "LGìƒí™œê±´ê°•", "í¬ìŠ¤ì½”ì¼€ì´ì¹¼",
        "ì—”ì”¨ì†Œí”„íŠ¸", "KT", "ì‚¼ì„±í™”ì¬", "SKë°”ì´ì˜¤ì‚¬ì´ì–¸ìŠ¤", "í•˜ì´ë¸Œ",
        "ì•„ëª¨ë ˆí¼ì‹œí”½", "ê¸°ì—…ì€í–‰", "SKì•„ì´ì´í…Œí¬ë†€ë¡œì§€", "í˜„ëŒ€ê¸€ë¡œë¹„ìŠ¤", "ë¡¯ë°ì¼€ë¯¸ì¹¼",
        "ë„·ë§ˆë¸”", "í•œêµ­ì¡°ì„ í•´ì–‘", "SKë°”ì´ì˜¤íŒœ", "LGë””ìŠ¤í”Œë ˆì´", "í•œì˜¨ì‹œìŠ¤í…œ"
    ]
    
    print(f"\nğŸ‡°ğŸ‡· í•œêµ­ ìƒìœ„ 50ê°œ ê¸°ì—… í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ë§")
    print(f"ì‹œê°€ì´ì•¡ ê¸°ì¤€ ì„ ì •ëœ ì£¼ìš” ê¸°ì—…ë“¤ì„ ìˆœì°¨ í¬ë¡¤ë§í•©ë‹ˆë‹¤")
    
    return run_multiple_companies_hybrid_crawl(
        company_list=top50_companies,
        pages=pages,
        headless=headless,
        delay_between_companies=30  # 30ì´ˆ ëŒ€ê¸°
    )


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì„ íƒì§€ ì¶”ê°€"""
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘           ë¸”ë¼ì¸ë“œ í¬ë¡¤ë§ ë„êµ¬ v2.0 - í•˜ì´ë¸Œë¦¬ë“œ ì²­í¬          â•‘
    â•‘                                                              â•‘
    â•‘  ì£¼ìš” íŠ¹ì§•:                                                   â•‘
    â•‘  â€¢ ì œëª©, ì¥ì , ë‹¨ì ì„ ë…ë¦½ ì²­í¬ë¡œ ë¶„ë¦¬                        â•‘
    â•‘  â€¢ ì¹´í…Œê³ ë¦¬ë³„ ì¥ì /ë‹¨ì  ì²­í¬ ìƒì„±                             â•‘
    â•‘  â€¢ is_positive ë©”íƒ€ë°ì´í„°ë¡œ ëª…í™•í•œ êµ¬ë¶„                       â•‘
    â•‘  â€¢ kss ê¸°ë°˜ ì •í™•í•œ ë¬¸ì¥ ë¶„í•                                  â•‘
    â•‘  â€¢ ë²¡í„° DB ìµœì í™”ëœ ë…ë¦½ ì €ì¥ êµ¬ì¡°                            â•‘
    â•‘                                                              â•‘
    â•‘  v2.0 ì‹ ê·œ ê¸°ëŠ¥:                                             â•‘
    â•‘  â€¢ ë‹¤ì¤‘ ê¸°ì—… ìˆœì°¨ í¬ë¡¤ë§ ì§€ì›                                â•‘
    â•‘  â€¢ í•œêµ­ ìƒìœ„ 50ê°œ ê¸°ì—… ì¼ê´„ í¬ë¡¤ë§                           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    try:
        print("\ní¬ë¡¤ë§ ëª¨ë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:")
        print("1. ë‹¨ì¼ ê¸°ì—… í¬ë¡¤ë§")
        print("2. ì—¬ëŸ¬ ê¸°ì—… í¬ë¡¤ë§ (ì§ì ‘ ì…ë ¥)")
        print("3. í•œêµ­ ìƒìœ„ 50ê°œ ê¸°ì—… ì¼ê´„ í¬ë¡¤ë§")
        
        choice = input("\nì„ íƒ (1-3): ").strip()
        
        if choice == "1":
            # ê¸°ì¡´ ë‹¨ì¼ ê¸°ì—… í¬ë¡¤ë§
            company = input("ê¸°ì—… ì½”ë“œ ì…ë ¥ (ì˜ˆ: NAVER): ").strip()
            pages = int(input("í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 25): ") or "25")
            headless = input("í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ? (y/N): ").lower() in ['y', 'yes']
            
            run_single_company_hybrid_crawl(company, pages, headless)
            
        elif choice == "2":
            # ì—¬ëŸ¬ ê¸°ì—… ì§ì ‘ ì…ë ¥
            print("\nê¸°ì—… ì½”ë“œë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: NAVER,ì‚¼ì„±ì „ì,ì¹´ì¹´ì˜¤)")
            companies_input = input("ê¸°ì—… ì½”ë“œë“¤: ").strip()
            company_list = [company.strip() for company in companies_input.split(',') if company.strip()]
            
            if not company_list:
                print("ìœ íš¨í•œ ê¸°ì—… ì½”ë“œê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return
            
            pages = int(input("í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 25): ") or "25")
            headless = input("í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ? (y/N): ").lower() in ['y', 'yes']
            delay = int(input("ê¸°ì—…ê°„ ëŒ€ê¸°ì‹œê°„(ì´ˆ) (ê¸°ë³¸ 30): ") or "30")
            
            run_multiple_companies_hybrid_crawl(company_list, pages, headless, delay)
            
        elif choice == "3":
            # ìƒìœ„ 50ê°œ ê¸°ì—… ì¼ê´„ í¬ë¡¤ë§
            pages = int(input("í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 25): ") or "25")
            headless = input("í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ? (y/N): ").lower() in ['y', 'yes']
            
            print(f"\nâš ï¸  ì£¼ì˜ì‚¬í•­:")
            print(f"â€¢ 50ê°œ ê¸°ì—… í¬ë¡¤ë§ì€ ìƒë‹¹í•œ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤ (ì˜ˆìƒ: 3-5ì‹œê°„)")
            print(f"â€¢ ê° ê¸°ì—…ê°„ 30ì´ˆì”© ëŒ€ê¸°í•˜ì—¬ ì„œë²„ ë¶€í•˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤")
            print(f"â€¢ ì¤‘ê°„ì— Ctrl+Cë¡œ ì¤‘ë‹¨ ê°€ëŠ¥í•˜ë©°, ê·¸ë•Œê¹Œì§€ì˜ ê²°ê³¼ëŠ” ì €ì¥ë©ë‹ˆë‹¤")
            
            confirm = input("\nê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower()
            if confirm in ['y', 'yes']:
                run_top50_korean_companies_crawl(pages, headless)
            else:
                print("í¬ë¡¤ë§ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        else:
            print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        
    except KeyboardInterrupt:
        print("\ní”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()