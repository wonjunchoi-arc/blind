# blind_review_crawler.py
"""
ë¸”ë¼ì¸ë“œ í¬ë¡¤ë§ ë„êµ¬ v3.2 - ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” ë²„ì „
- í¬ë¡¤ë§ê³¼ AI ë¶„ë¥˜ë¥¼ ë¶„ë¦¬í•˜ì—¬ íš¨ìœ¨ì„± í–¥ìƒ
- ëª¨ë“  ì²­í¬ë¥¼ ìˆ˜ì§‘í•œ í›„ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ë¡œ AI ë¶„ë¥˜
- OpenAI API í˜¸ì¶œ íšŸìˆ˜ ëŒ€í­ ê°ì†Œ
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from bs4 import BeautifulSoup
import time
import pandas as pd
import json
import re
import logging
import os
import glob
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import asdict
from dotenv import load_dotenv
from tqdm import tqdm

# ê¸°ì¡´ ëª¨ë“ˆ import
from enhanced_category_processor import CategorySpecificProcessor, VectorDBOptimizer
from keyword_dictionary import korean_keywords

# Settings import (í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì‚¬ìš©)
import sys
from pathlib import Path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

try:
    from blindinsight.models.base import settings
    SETTINGS_AVAILABLE = True
except ImportError:
    SETTINGS_AVAILABLE = False

# ë¡œê¹… ì„¤ì • (ê°„ì†Œí™”)
logging.basicConfig(
    level=logging.WARNING,
    format='%(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('blind_crawler.log', encoding='utf-8'),
    ]
)
logger = logging.getLogger(__name__)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class BlindReviewCrawler:
    """ê°œì„ ëœ ë¸”ë¼ì¸ë“œ ë¦¬ë·° í¬ë¡¤ëŸ¬ - ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”"""
    
    def __init__(self, headless=False, wait_timeout=10, output_dir="./data/vectordb", 
                 use_ai_classification=True, openai_api_key=None, enable_spell_check=True):
        self.wait_timeout = wait_timeout
        self.driver = None
        self.wait = None
        self.output_dir = output_dir
        self.is_logged_in = False
        
        # ë¶„ë¥˜ ë°©ì‹ ì„¤ì •
        self.use_ai_classification = use_ai_classification
        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
        self.enable_spell_check = enable_spell_check
        
        # AI ë¶„ë¥˜ë¥¼ ì›í•˜ì§€ë§Œ API í‚¤ê°€ ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ë¶„ë¥˜ë¡œ ìë™ ì „í™˜
        if self.use_ai_classification and not self.openai_api_key:
            print("âš ï¸ OpenAI API í‚¤ê°€ ì—†ì–´ í‚¤ì›Œë“œ ë¶„ë¥˜ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            self.use_ai_classification = False
        
        # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.category_processor = CategorySpecificProcessor(
            openai_api_key=self.openai_api_key if self.use_ai_classification else None,
            enable_spell_check=self.enable_spell_check if self.use_ai_classification else False
        )
        self.vectordb_optimizer = VectorDBOptimizer()
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # ê°„ì†Œí™”ëœ ì²˜ë¦¬ ê²°ê³¼ ì¶”ì 
        self.results = {
            "reviews_processed": 0,
            "chunks_created": 0,
            "api_calls_saved": 0,
            "category_counts": {
                "career_growth": 0,
                "salary_benefits": 0,
                "work_life_balance": 0,
                "company_culture": 0,
                "management": 0
            }
        }
        
        # Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™”
        self._initialize_driver(headless)
        
        # ë¶„ë¥˜ ë°©ì‹ ì¶œë ¥
        classification_method = "AI ë°°ì¹˜ ë¶„ë¥˜" if self.use_ai_classification else "í‚¤ì›Œë“œ ë¶„ë¥˜"
        print(f"ğŸ” ë¶„ë¥˜ ë°©ì‹: {classification_method}")
    
    def _initialize_driver(self, headless):
        """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        chrome_options = Options()
        if headless:
            chrome_options.add_argument("--headless")
        
        # ê¸°ë³¸ í•„ìˆ˜ ì„¤ì •
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-web-security")
        chrome_options.add_argument("--disable-features=VizDisplayCompositor")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # User-Agent ì„¤ì •
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36")
        chrome_options.add_argument("--window-size=1920,1080")
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            
            # ìë™í™” íƒì§€ ìš°íšŒ ìŠ¤í¬ë¦½íŠ¸
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined,
                    });
                '''
            })
            
            self.wait = WebDriverWait(self.driver, self.wait_timeout)
            
        except Exception as e:
            print(f"âŒ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def login_wait(self, url):
        """ë¡œê·¸ì¸ ëŒ€ê¸° ë° ì²˜ë¦¬"""
        try:
            print(f"ğŸŒ í˜ì´ì§€ ì ‘ì† ì¤‘: {url}")
            self.driver.get(url)
            time.sleep(5)
            
            # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­ ì‹œë„
            try:
                login_btn = self.wait.until(
                    EC.element_to_be_clickable((By.CLASS_NAME, "btn_signin"))
                )
                self.driver.execute_script("arguments[0].click();", login_btn)
            except Exception:
                pass
            
            time.sleep(3)
            
            print("\n" + "="*50)
            print("ğŸ” ë¸”ë¼ì¸ë“œ ë¡œê·¸ì¸ í•„ìš”")
            print("1. ë¸”ë¼ì¸ë“œ ì•±ì—ì„œ ë¡œê·¸ì¸")
            print("2. 'ë”ë³´ê¸°' â†’ 'ë¸”ë¼ì¸ë“œ ì›¹ ë¡œê·¸ì¸' í´ë¦­")
            print("3. ì¸ì¦ë²ˆí˜¸ ì…ë ¥")
            print("="*50)
            
            input("ë¡œê·¸ì¸ ì™„ë£Œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”: ")
            self.is_logged_in = True
            print("âœ… ë¡œê·¸ì¸ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ë¡œê·¸ì¸ ê³¼ì • ì˜¤ë¥˜: {e}")
            raise
    
    def extract_review_data(self, element):
        """ê°œë³„ ë¦¬ë·°ì—ì„œ ë°ì´í„° ì¶”ì¶œ - ì§ë¬´/ì—°ë„ ì •ë³´ ì¶”ê°€"""
        try:
            # í‰ì  ì •ë³´ ì¶”ì¶œ
            rating_element = element.find_element(By.CLASS_NAME, "rating")
            
            # ìƒì„¸ í‰ì  ë³´ê¸° í´ë¦­
            try:
                more_rating_btn = rating_element.find_element(By.CLASS_NAME, "more_rating")
                more_rating_btn.click()
                time.sleep(0.5)
            except NoSuchElementException:
                pass
            
            # ì´ì  ì¶”ì¶œ
            try:
                score_element = rating_element.find_element(By.CLASS_NAME, "num")
                total_score = float(score_element.text.split("\n")[1])
            except (ValueError, IndexError):
                total_score = 0.0
            
            # ìƒì„¸ ì ìˆ˜ ì¶”ì¶œ
            detail_scores = ["0"] * 5
            try:
                detail_elements = element.find_elements(
                    By.CSS_SELECTOR, 
                    "div.review_item_inr > div.rating > div.more_rating > span > span > span > div.ly_rating > div.rating_wp > span.desc > i.blind"
                )
                for i, detail_elem in enumerate(detail_elements[:5]):
                    detail_scores[i] = detail_elem.text
            except Exception:
                pass
            
            # ì œëª© ì¶”ì¶œ
            try:
                title_element = element.find_element(By.CSS_SELECTOR, "div.review_item_inr > h3.rvtit > a")
                title = self.clean_text(title_element.text)
            except NoSuchElementException:
                title = "ì œëª© ì—†ìŒ"
            
            # ì§ì› ìœ í˜•, ì§ë¬´, ì—°ë„ ì¶”ì¶œ (auth í´ë˜ìŠ¤ì—ì„œ)
            try:
                auth_element = element.find_element(By.CSS_SELECTOR, "div.review_item_inr > div.auth")
                auth_text = auth_element.text
                
                # ê¸°ë³¸ê°’ ì„¤ì •
                status = "ì •ë³´ ì—†ìŒ"
                position = "ì •ë³´ ì—†ìŒ"
                year = "ì •ë³´ ì—†ìŒ"
                
                # auth_text í˜•íƒœ 1: "í˜„ì§ì›\nì—”ì§€ë‹ˆì–´ë§\n2020.03.10"
                # auth_text í˜•íƒœ 2: "í˜„ì§ì› Â· j********* Â· í•˜ë“œì›¨ì–´ ì—”ì§€ë‹ˆì–´ - 2025.02.23"
                
                if 'Â·' in auth_text:
                    # í˜•íƒœ 2: Â· êµ¬ë¶„ìë¡œ íŒŒì‹±
                    auth_parts = [part.strip() for part in auth_text.split('Â·') if part.strip()]
                    
                    if len(auth_parts) >= 1:
                        # "Verified User\ní˜„ì§ì›"ì—ì„œ "í˜„ì§ì›"ë§Œ ì¶”ì¶œ
                        raw_status = auth_parts[0].strip()
                        if '\n' in raw_status:
                            status = raw_status.split('\n')[-1].strip()  # ë§ˆì§€ë§‰ ë¶€ë¶„ë§Œ ê°€ì ¸ì˜¤ê¸°
                        else:
                            status = raw_status
                    
                    if len(auth_parts) >= 3:
                        # ì§ë¬´ ì •ë³´ëŠ” ë§ˆì§€ë§‰ ë¶€ë¶„ì—ì„œ ì¶”ì¶œ (ì˜ˆ: "ìƒì‚°ì—”ì§€ë‹ˆì–´Â·ìƒì‚°ê´€ë¦¬ - 2024.02.06")
                        job_date_part = auth_parts[2].strip()
                        
                        # ë‚ ì§œ ë¶€ë¶„ ì œê±°í•˜ê³  ì§ë¬´ë§Œ ì¶”ì¶œ
                        if '-' in job_date_part:
                            # ë‚ ì§œ íŒ¨í„´ì„ ì°¾ì•„ì„œ ê·¸ ë¶€ë¶„ë§Œ ì œê±°
                            date_match = re.search(r'\s*-\s*\d{4}\.\d{2}\.\d{2}', job_date_part)
                            if date_match:
                                position = job_date_part[:date_match.start()].strip()
                                # ì—°ë„ ì¶”ì¶œ
                                year_match = re.search(r'(\d{4})', date_match.group())
                                if year_match:
                                    year = year_match.group(1)
                            else:
                                # ì •ê·œì‹ìœ¼ë¡œ ë‚ ì§œ íŒ¨í„´ ì œê±° (í´ë°± ë°©ì‹)
                                position = re.sub(r'\s*-\s*\d{4}\.?\d{0,2}\.?\d{0,2}.*', '', job_date_part).strip()
                                year_match = re.search(r'(\d{4})', job_date_part)
                                if year_match:
                                    year = year_match.group(1)
                        else:
                            position = job_date_part
                            
                else:
                    # í˜•íƒœ 1: ê°œí–‰ êµ¬ë¶„ìë¡œ íŒŒì‹±
                    auth_lines = [line.strip() for line in auth_text.split('\n') if line.strip()]
                    
                    if len(auth_lines) >= 1:
                        # "Verified User"ê°™ì€ ë¶€ë¶„ì„ ì œì™¸í•˜ê³  ì‹¤ì œ ì§ì› ìƒíƒœë§Œ ì¶”ì¶œ
                        raw_status = auth_lines[0]
                        if 'í˜„ì§ì›' in raw_status:
                            status = 'í˜„ì§ì›'
                        elif 'ì „ì§ì›' in raw_status:
                            status = 'ì „ì§ì›'
                        else:
                            status = raw_status
                    
                    if len(auth_lines) >= 2:
                        position = auth_lines[1]  # ì§ë¬´
                    
                    if len(auth_lines) >= 3:
                        # ì—°ë„ ì¶”ì¶œ (2020.03.10 í˜•íƒœì—ì„œ 2020ë§Œ ì¶”ì¶œ)
                        date_text = auth_lines[2]
                        year_match = re.search(r'(\d{4})', date_text)
                        if year_match:
                            year = year_match.group(1)
                        
            except (NoSuchElementException, IndexError):
                status = "ì •ë³´ ì—†ìŒ"
                position = "ì •ë³´ ì—†ìŒ"
                year = "ì •ë³´ ì—†ìŒ"
            
            # ì¥ì /ë‹¨ì  ì¶”ì¶œ
            pros = "ì •ë³´ ì—†ìŒ"
            cons = "ì •ë³´ ì—†ìŒ"
            
            try:
                parag_element = element.find_element(By.CSS_SELECTOR, ".parag")
                full_text = parag_element.text
                
                lines = full_text.split('\n')
                
                pros_started = False
                cons_started = False
                pros_lines = []
                cons_lines = []
                
                for line in lines:
                    line = line.strip()
                    if not line:
                        continue
                        
                    if 'ì¥ì ' in line and len(line) < 10:
                        pros_started = True
                        cons_started = False
                        continue
                    elif 'ë‹¨ì ' in line and len(line) < 10:
                        cons_started = True
                        pros_started = False
                        continue
                    elif any(keyword in line for keyword in ['ì¶”ì²œ', 'í‰ì ', 'íšŒì‚¬ì—ê²Œ', 'ê²½ì˜ì§„ì—ê²Œ']):
                        pros_started = False
                        cons_started = False
                        continue
                    
                    if pros_started and line:
                        pros_lines.append(line)
                    elif cons_started and line:
                        cons_lines.append(line)
                
                if pros_lines:
                    pros = self.clean_text(' '.join(pros_lines))
                if cons_lines:
                    cons = self.clean_text(' '.join(cons_lines))
                    
            except Exception:
                pass
            
            return [
                total_score,
                detail_scores[0],
                detail_scores[1], 
                detail_scores[2],
                detail_scores[3],
                detail_scores[4],
                title,
                status,
                position,      # ìƒˆë¡œ ì¶”ê°€: ì§ë¬´
                year,          # ìƒˆë¡œ ì¶”ê°€: ì—°ë„
                pros,
                cons
            ]
            
        except Exception as e:
            return [0.0, "0", "0", "0", "0", "0", "ì˜¤ë¥˜", "ì˜¤ë¥˜", "ì˜¤ë¥˜", "ì˜¤ë¥˜", "ì¶”ì¶œ ì‹¤íŒ¨", "ì¶”ì¶œ ì‹¤íŒ¨"]
    
    def clean_text(self, text):
        """ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text:
            return ""
        
        # í•œê¸€ ìëª¨ ì œê±°
        pattern = '([ã„±-ã…ã…-ã…£]+)'
        text = re.sub(pattern=pattern, repl='', string=text)
        
        # HTML íƒœê·¸ ì œê±°
        pattern = '<[^>]*>'
        text = re.sub(pattern=pattern, repl='', string=text)
        
        # ê¸°ë³¸ íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        pattern = '[^\w\sê°€-í£.,!?()-]'
        text = re.sub(pattern=pattern, repl=' ', string=text)
        
        # ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ
        text = text.replace('\r', '').replace('\n', ' ')
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def crawl_company_reviews(self, company_code: str, pages: int = 25):
        """íšŒì‚¬ ë¦¬ë·° í¬ë¡¤ë§ - ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”"""
        
        print(f"\nğŸš€ {company_code} í¬ë¡¤ë§ ì‹œì‘")
        
        base_url = f"https://www.teamblind.com/kr/company/{company_code}/reviews"
        
        if not self.is_logged_in:
            self.login_wait(base_url)
        
        # 1ë‹¨ê³„: ì „ì²´ ë¦¬ë·° ìˆ˜ì§‘ (ë³€ê²½ ì—†ìŒ)
        all_reviews = []
        
        print(f"ğŸ“„ í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
        for page in range(1, pages + 1):
            try:
                target_url = f"{base_url}?page={page}"
                
                self.driver.get(target_url)
                time.sleep(3)
                
                review_elements = self.driver.find_elements(By.CLASS_NAME, "review_item")
                
                if not review_elements:
                    continue
                
                for idx, element in enumerate(review_elements):
                    try:
                        review_data = self.extract_review_data(element)
                        all_reviews.append(review_data)
                    except Exception:
                        continue
                
                time.sleep(2)
                
            except Exception:
                continue
        
        if not all_reviews:
            print("âŒ ì¶”ì¶œëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"âœ… ì´ {len(all_reviews)}ê°œ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ")
        
        # 2ë‹¨ê³„: ê°œì„ ëœ ë°°ì¹˜ ì²˜ë¦¬
        return self._process_reviews_with_batch_optimization(company_code, all_reviews)
    
    def _process_reviews_with_batch_optimization(self, company_code: str, all_reviews: List) -> bool:
        """ê°œì„ ëœ ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ - ì§ë¬´/ì—°ë„ í•„ë“œ ì¶”ê°€"""
        
        print(f"\nğŸ“Š ë¦¬ë·° ì „ì²˜ë¦¬ ì¤‘...")
        
        # 1ë‹¨ê³„: ëª¨ë“  ë¦¬ë·°ì—ì„œ ì²­í¬ ìƒì„± (ë¶„ë¥˜ ì—†ì´)
        all_chunk_data = []  # ë¶„ë¥˜ ì „ ì²­í¬ ì •ë³´ ì €ì¥
        all_chunk_contents = []  # AIì— ë³´ë‚¼ í…ìŠ¤íŠ¸ë§Œ ì €ì¥
        
        for idx, raw_review in enumerate(tqdm(all_reviews, desc="ì²­í¬ ìƒì„±", unit="ë¦¬ë·°")):
            try:
                # ë¦¬ë·° ë°ì´í„° êµ¬ì¡°í™” (í•„ë“œ ì¶”ê°€)
                review_data = {
                    "íšŒì‚¬": company_code,
                    "ì´ì ": raw_review[0],
                    "ì»¤ë¦¬ì–´í–¥ìƒ": raw_review[1], 
                    "ì›Œë¼ë°¸": raw_review[2],
                    "ê¸‰ì—¬ë³µì§€": raw_review[3],
                    "ì‚¬ë‚´ë¬¸í™”": raw_review[4],
                    "ê²½ì˜ì§„": raw_review[5],
                    "ì œëª©": raw_review[6],
                    "ì§ì›ìœ í˜•_ì›ë³¸": raw_review[7],
                    "ì§ë¬´": raw_review[8],        # ìƒˆë¡œ ì¶”ê°€
                    "ì—°ë„": raw_review[9],        # ìƒˆë¡œ ì¶”ê°€
                    "ì§ì›ìœ í˜•": raw_review[7],  # í˜„ì§ì›/ì „ì§ì› ìƒíƒœ ì¶”ê°€
                    "ì¥ì ": raw_review[10],       # ì¸ë±ìŠ¤ ì¡°ì •
                    "ë‹¨ì ": raw_review[11],       # ì¸ë±ìŠ¤ ì¡°ì •
                    "id": f"review_{company_code}_{idx:04d}"
                }
                
                # ë¶„ë¥˜ ì—†ì´ ì²­í¬ë§Œ ìƒì„±
                chunk_groups = self.category_processor.create_chunks_without_classification(review_data)
                
                # ì²­í¬ ë°ì´í„°ì™€ ë‚´ìš© ë¶„ë¦¬ ì €ì¥
                for chunk_type, chunks_info in chunk_groups.items():
                    for chunk_info in chunks_info:
                        all_chunk_data.append(chunk_info)
                        all_chunk_contents.append(chunk_info['content'])
                
            except Exception:
                continue
        
        if not all_chunk_contents:
            print("âŒ ìƒì„±ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"ğŸ“‹ ì´ {len(all_chunk_contents)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        
        # ë‚˜ë¨¸ì§€ ì²˜ë¦¬ëŠ” ê¸°ì¡´ê³¼ ë™ì¼...
        # (ë¶„ë¥˜, ë§¤í•‘, ì €ì¥ ë¡œì§ì€ ë³€ê²½ ì—†ìŒ)
        
        # 2ë‹¨ê³„: ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ë¶„ë¥˜ (AI ì‚¬ìš©ì‹œì—ë§Œ)
        classification_results = []
        
        if self.use_ai_classification and self.category_processor.text_processor:
            try:
                print(f"ğŸ§  AI ë°°ì¹˜ ë¶„ë¥˜ ì‹œì‘...")
                
                # ì˜ˆìƒ API í˜¸ì¶œ íšŸìˆ˜ ê³„ì‚°
                if SETTINGS_AVAILABLE:
                    batch_size = settings.ai_batch_size
                else:
                    batch_size = int(os.getenv("AI_BATCH_SIZE", "30"))
                    
                expected_api_calls = (len(all_chunk_contents) + batch_size - 1) // batch_size
                individual_calls_saved = len(all_reviews) - expected_api_calls
                
                print(f"   - ì²­í¬ ìˆ˜: {len(all_chunk_contents)}ê°œ")
                print(f"   - ì˜ˆìƒ API í˜¸ì¶œ: {expected_api_calls}íšŒ")
                print(f"   - ì ˆì•½ëœ API í˜¸ì¶œ: {individual_calls_saved}íšŒ")
                
                # ë°°ì¹˜ ë¶„ë¥˜ ì‹¤í–‰
                classification_results = self.category_processor.text_processor.process_chunks_batch(
                    all_chunk_contents, batch_size=batch_size
                )
                
                self.results["api_calls_saved"] = individual_calls_saved
                
            except Exception as e:
                print(f"âš ï¸ AI ë¶„ë¥˜ ì‹¤íŒ¨, í‚¤ì›Œë“œ ë¶„ë¥˜ë¡œ í´ë°±: {e}")
                classification_results = [
                    self.category_processor._classify_with_keywords_fallback(content)
                    for content in all_chunk_contents
                ]
        else:
            # í‚¤ì›Œë“œ ë¶„ë¥˜
            print(f"ğŸ”¤ í‚¤ì›Œë“œ ë¶„ë¥˜ ì¤‘...")
            classification_results = []
            for content in tqdm(all_chunk_contents, desc="í‚¤ì›Œë“œ ë¶„ë¥˜", unit="ì²­í¬"):
                result = self.category_processor._classify_with_keywords_fallback(content)
                classification_results.append(result)
        
        # 3ë‹¨ê³„: ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì²­í¬ ë°ì´í„°ì— ë§¤í•‘
        print(f"ğŸ”— ê²°ê³¼ ë§¤í•‘ ì¤‘...")
        final_chunks = self._map_classification_results_to_chunks(
            all_chunk_data, classification_results
        )
        
        # 4ë‹¨ê³„: ë²¡í„° DB ìµœì í™” ë° ì €ì¥
        print(f"ğŸ’¾ íŒŒì¼ ì €ì¥ ì¤‘...")
        optimized_chunks = self.vectordb_optimizer.optimize_chunks_for_vectordb(final_chunks)
        
        file_path = self.vectordb_optimizer.save_vectordb_file(
            company=company_code, 
            optimized_chunks=optimized_chunks, 
            output_dir=self.output_dir,
            category_processor=self.category_processor
        )
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.results["reviews_processed"] = len(all_reviews)
        self.results["chunks_created"] = len(final_chunks)
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        for chunk in final_chunks:
            category = chunk.get('category', 'career_growth')
            if category in self.results["category_counts"]:
                self.results["category_counts"][category] += 1
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_batch_optimization_summary(company_code, file_path)
        
        return True
    
    def _map_classification_results_to_chunks(self, chunk_data_list, classification_results):
        """ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì²­í¬ ë°ì´í„°ì— ë§¤í•‘"""
        
        final_chunks = []
        
        for i, (chunk_data, classification) in enumerate(zip(chunk_data_list, classification_results)):
            # 1ìˆœìœ„ ì¹´í…Œê³ ë¦¬ë¡œ ì²­í¬ ìƒì„±
            primary_chunk = self.category_processor.create_final_chunk(
                chunk_data, classification, "primary"
            )
            final_chunks.append(primary_chunk)
            
            # 2ìˆœìœ„ ì¹´í…Œê³ ë¦¬ ì²­í¬ ìƒì„± (ì‹ ë¢°ë„ ì¡°ê±´ ë§Œì¡±ì‹œ)
            #if (classification.get("secondary_category") and 
             #   classification.get("secondary_confidence", 0) >= 0.3 and
              #  classification["secondary_category"] != classification["primary_category"]):
                
               # secondary_chunk = self.category_processor.create_final_chunk(
                #    chunk_data, classification, "secondary"
                #)
                #final_chunks.append(secondary_chunk)
        
        return final_chunks
    
    def _print_batch_optimization_summary(self, company_code: str, file_path: str):
        """ë°°ì¹˜ ìµœì í™” ê²°ê³¼ ì¶œë ¥"""
        
        print(f"\n{'='*50}")
        print(f"ğŸ‰ {company_code} í¬ë¡¤ë§ ì™„ë£Œ (ë°°ì¹˜ ìµœì í™”)")
        print(f"{'='*50}")
        
        # ê¸°ë³¸ í†µê³„
        print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        print(f"  - ì²˜ë¦¬ëœ ë¦¬ë·°: {self.results['reviews_processed']}ê°œ")
        print(f"  - ìƒì„±ëœ ì²­í¬: {self.results['chunks_created']}ê°œ")
        
        # ë°°ì¹˜ ìµœì í™” íš¨ê³¼
        if self.use_ai_classification and self.results['api_calls_saved'] > 0:
            print(f"  - ì ˆì•½ëœ API í˜¸ì¶œ: {self.results['api_calls_saved']}íšŒ")
            efficiency_improvement = (self.results['api_calls_saved'] / self.results['reviews_processed']) * 100
            print(f"  - íš¨ìœ¨ì„± ê°œì„ : {efficiency_improvement:.1f}%")
        
        # ë¶„ë¥˜ ë°©ì‹ í‘œì‹œ
        classification_method = "AI ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ë¶„ë¥˜" if self.use_ai_classification else "í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜"
        print(f"  - ë¶„ë¥˜ ë°©ì‹: {classification_method}")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì²­í¬ ìˆ˜
        category_names_kr = {
            "career_growth": "ì»¤ë¦¬ì–´ í–¥ìƒ",
            "salary_benefits": "ê¸‰ì—¬ ë° ë³µì§€", 
            "work_life_balance": "ì—…ë¬´ì™€ ì‚¶ì˜ ê· í˜•",
            "company_culture": "ì‚¬ë‚´ ë¬¸í™”",
            "management": "ê²½ì˜ì§„"
        }
        
        print(f"\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì²­í¬ ìˆ˜:")
        for category, count in self.results["category_counts"].items():
            category_kr = category_names_kr.get(category, category)
            print(f"  - {category_kr}: {count}ê°œ")
        
        # íŒŒì¼ ì •ë³´
        print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
        print(f"  - {os.path.basename(file_path)}")
        
        print(f"\nâœ… ë°°ì¹˜ ìµœì í™” ì™„ë£Œ!")
        print(f"{'='*50}\n")
    
    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()


def run_single_company_crawl(company_code: str, pages: int = 25, headless: bool = False, 
                            use_ai_classification: bool = True, openai_api_key: str = None, 
                            enable_spell_check: bool = True):
    """ë‹¨ì¼ ê¸°ì—… í¬ë¡¤ë§ ì‹¤í–‰"""
    
    print(f"\në¸”ë¼ì¸ë“œ í¬ë¡¤ëŸ¬ v3.2 - ë°°ì¹˜ ìµœì í™”")
    print(f"ğŸ¯ ëŒ€ìƒ ê¸°ì—…: {company_code}")
    print(f"ğŸ“„ í¬ë¡¤ë§ í˜ì´ì§€: {pages}")
    
    crawler = BlindReviewCrawler(
        headless=headless,
        output_dir="./data/vectordb",
        use_ai_classification=use_ai_classification,
        openai_api_key=openai_api_key,
        enable_spell_check=enable_spell_check
    )
    
    try:
        success = crawler.crawl_company_reviews(company_code, pages)
        return success
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return False
    finally:
        crawler.close()


def run_multiple_companies_crawl(company_list: List[str], pages: int = 25, 
                                headless: bool = False, delay_between_companies: int = 30,
                                use_ai_classification: bool = True, openai_api_key: str = None, 
                                enable_spell_check: bool = True):
    """ì—¬ëŸ¬ ê¸°ì—… ìˆœì°¨ í¬ë¡¤ë§ ì‹¤í–‰"""
    
    print(f"\në¸”ë¼ì¸ë“œ ë‹¤ì¤‘ ê¸°ì—… í¬ë¡¤ëŸ¬ v3.2 - ë°°ì¹˜ ìµœì í™”")
    print(f"ğŸ¯ ëŒ€ìƒ ê¸°ì—…: {len(company_list)}ê°œ")
    print(f"ğŸ“„ í¬ë¡¤ë§ í˜ì´ì§€: {pages}")
    
    # í¬ë¡¤ë§ ê²°ê³¼ ì¶”ì 
    results = {
        "success": [],
        "failed": [],
        "total_companies": len(company_list),
        "total_api_calls_saved": 0
    }
    
    # ë‹¨ì¼ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = BlindReviewCrawler(
        headless=headless,
        output_dir="./data/vectordb",
        use_ai_classification=use_ai_classification,
        openai_api_key=openai_api_key,
        enable_spell_check=enable_spell_check
    )
    
    try:
        print(f"\n{'='*50}")
        print(f"ğŸš€ ë°°ì¹˜ ìµœì í™” ë‹¤ì¤‘ ê¸°ì—… í¬ë¡¤ë§ ì‹œì‘")
        print(f"{'='*50}")
        
        for idx, company_code in enumerate(company_list):
            try:
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"\n[{idx+1}/{len(company_list)}] {current_time} - {company_code}")
                
                # í¬ë¡¤ë§ ì‹¤í–‰
                success = crawler.crawl_company_reviews(company_code, pages)
                
                if success:
                    results["success"].append(company_code)
                    results["total_api_calls_saved"] += crawler.results.get("api_calls_saved", 0)
                else:
                    results["failed"].append(company_code)
                
                # ëŒ€ê¸° (ë§ˆì§€ë§‰ ê¸°ì—…ì´ ì•„ë‹Œ ê²½ìš°)
                if idx < len(company_list) - 1:
                    print(f"â±ï¸ {delay_between_companies}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(delay_between_companies)
                    
            except Exception as e:
                print(f"âŒ {company_code} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                results["failed"].append(company_code)
                
                if idx < len(company_list) - 1:
                    time.sleep(delay_between_companies)
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        _print_multiple_crawling_summary_optimized(results)
        return results
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        _print_multiple_crawling_summary_optimized(results)
        return results
    except Exception as e:
        print(f"âŒ ë‹¤ì¤‘ í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return results
    finally:
        crawler.close()


def _print_multiple_crawling_summary_optimized(results: Dict):
    """ë°°ì¹˜ ìµœì í™”ëœ ë‹¤ì¤‘ í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    
    print(f"\n{'='*50}")
    print(f"ğŸ ë°°ì¹˜ ìµœì í™” ë‹¤ì¤‘ ê¸°ì—… í¬ë¡¤ë§ ì™„ë£Œ")
    print(f"{'='*50}")
    
    print(f"ğŸ“Š ì „ì²´ ê²°ê³¼:")
    print(f"  - ì´ ëŒ€ìƒ ê¸°ì—…: {results['total_companies']}ê°œ")
    print(f"  - ì„±ê³µ: {len(results['success'])}ê°œ")
    print(f"  - ì‹¤íŒ¨: {len(results['failed'])}ê°œ")
    print(f"  - ì„±ê³µë¥ : {len(results['success'])/results['total_companies']*100:.1f}%")
    
    # ë°°ì¹˜ ìµœì í™” íš¨ê³¼
    if results.get("total_api_calls_saved", 0) > 0:
        print(f"  - ì´ ì ˆì•½ëœ API í˜¸ì¶œ: {results['total_api_calls_saved']}íšŒ")
        print(f"  - ì˜ˆìƒ ë¹„ìš© ì ˆì•½: ${results['total_api_calls_saved'] * 0.002:.2f}")
    
    if results["success"]:
        print(f"\nâœ… ì„±ê³µí•œ ê¸°ì—…:")
        for i, company in enumerate(results["success"]):
            print(f"  {i+1:2d}. {company}")
    
    if results["failed"]:
        print(f"\nâŒ ì‹¤íŒ¨í•œ ê¸°ì—…:")
        for i, company in enumerate(results["failed"]):
            print(f"  {i+1:2d}. {company}")
    
    print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
    print(f"  - ./data/vectordb/ ë””ë ‰í† ë¦¬ì— ê° ê¸°ì—…ë³„ íŒŒì¼ ì €ì¥")
    print(f"{'='*50}\n")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                    ë¸”ë¼ì¸ë“œ í¬ë¡¤ë§ ë„êµ¬ v3.2                     â•‘
    â•‘                     ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” ë²„ì „                        â•‘
    â•‘                                                                â•‘
    â•‘  â€¢ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ (ê¸°ë³¸)                                       â•‘
    â•‘  â€¢ AI ê¸°ë°˜ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ë¶„ë¥˜ (OpenAI API í‚¤ í•„ìš”)                 â•‘  
    â•‘  â€¢ API í˜¸ì¶œ íšŸìˆ˜ ìµœëŒ€ 90% ì ˆì•½                                   â•‘
    â•‘  â€¢ ê°„ì†Œí™”ëœ ì¶œë ¥ê³¼ ì§„í–‰ë¥  í‘œì‹œ                                    â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    try:
        print("í¬ë¡¤ë§ ëª¨ë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:")
        print("1. ë‹¨ì¼ ê¸°ì—… í¬ë¡¤ë§")
        print("2. ì—¬ëŸ¬ ê¸°ì—… í¬ë¡¤ë§ (ì§ì ‘ ì…ë ¥)")
        print("3. í•œêµ­ ìƒìœ„ 50ê°œ ê¸°ì—… ì¼ê´„ í¬ë¡¤ë§")
        
        choice = input("\nì„ íƒ (1-3): ").strip()
        
        # ë¶„ë¥˜ ë°©ì‹ ì„ íƒ
        print("\në¶„ë¥˜ ë°©ì‹ì„ ì„ íƒí•´ì£¼ì„¸ìš”:")
        print("1. í‚¤ì›Œë“œ ë¶„ë¥˜ (ë¹ ë¦„, API ë¹„ìš© ì—†ìŒ)")
        print("2. AI ë°°ì¹˜ ë¶„ë¥˜ (ì •í™•í•¨, OpenAI API í‚¤ í•„ìš”, ë¹„ìš© ìµœì í™”)")
        
        classification_choice = input("ì„ íƒ (1-2, ê¸°ë³¸ê°’: 1): ").strip()
        use_ai_classification = classification_choice == "2"
        
        # API í‚¤ ì„¤ì • (AI ë¶„ë¥˜ ì„ íƒì‹œ)
        openai_api_key = None
        enable_spell_check = False
        
        if use_ai_classification:
            openai_api_key = os.getenv('OPENAI_API_KEY')
            if not openai_api_key:
                print("\nâš ï¸ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                api_input = input("API í‚¤ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ Enterë¡œ í‚¤ì›Œë“œ ë¶„ë¥˜ ì‚¬ìš©: ").strip()
                if api_input:
                    openai_api_key = api_input
                else:
                    use_ai_classification = False
            
            if use_ai_classification:
                enable_spell_check = input("ë§ì¶¤ë²• ê²€ì‚¬ ì‚¬ìš©? (y/N): ").lower() in ['y', 'yes']
                print("\nğŸ’¡ ë°°ì¹˜ ìµœì í™” íš¨ê³¼:")
                print("  - ê°œë³„ ë¦¬ë·° ì²˜ë¦¬ ëŒ€ë¹„ API í˜¸ì¶œ 90% ì ˆì•½")
                print("  - ëŒ€ìš©ëŸ‰ ë°°ì¹˜ë¡œ ì²˜ë¦¬ ì†ë„ í–¥ìƒ")
                print("  - ë¹„ìš© íš¨ìœ¨ì ì¸ AI ë¶„ë¥˜")
        
        if choice == "1":
            # ë‹¨ì¼ ê¸°ì—… í¬ë¡¤ë§
            company = input("\nê¸°ì—… ì½”ë“œ ì…ë ¥ (ì˜ˆ: NAVER): ").strip()
            pages = int(input("í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 25): ") or "25")
            headless = input("í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ? (y/N): ").lower() in ['y', 'yes']
            
            run_single_company_crawl(
                company_code=company, 
                pages=pages, 
                headless=headless,
                use_ai_classification=use_ai_classification,
                openai_api_key=openai_api_key,
                enable_spell_check=enable_spell_check
            )
            
        elif choice == "2":
            # ì—¬ëŸ¬ ê¸°ì—… ì§ì ‘ ì…ë ¥
            print("\nê¸°ì—… ì½”ë“œë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: NAVER,ì‚¼ì„±ì „ì,ì¹´ì¹´ì˜¤)")
            companies_input = input("ê¸°ì—… ì½”ë“œë“¤: ").strip()
            company_list = [company.strip() for company in companies_input.split(',') if company.strip()]
            
            if not company_list:
                print("âŒ ìœ íš¨í•œ ê¸°ì—… ì½”ë“œê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return
            
            pages = int(input("í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 25): ") or "25")
            headless = input("í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ? (y/N): ").lower() in ['y', 'yes']
            delay = int(input("ê¸°ì—…ê°„ ëŒ€ê¸°ì‹œê°„(ì´ˆ) (ê¸°ë³¸ 30): ") or "30")
            
            run_multiple_companies_crawl(
                company_list=company_list, 
                pages=pages, 
                headless=headless,
                delay_between_companies=delay,
                use_ai_classification=use_ai_classification,
                openai_api_key=openai_api_key,
                enable_spell_check=enable_spell_check
            )
            
        elif choice == "3":
            # ìƒìœ„ 50ê°œ ê¸°ì—… ì¼ê´„ í¬ë¡¤ë§
            top50_companies = [
                "ì‚¼ì„±ì „ì", "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "SKí•˜ì´ë‹‰ìŠ¤", "ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤", "NAVER",
                "LGí™”í•™", "í˜„ëŒ€ì°¨", "ì‚¼ì„±SDI", "ì¹´ì¹´ì˜¤", "ê¸°ì•„",
                "POSCOí™€ë”©ìŠ¤", "KBê¸ˆìœµ", "SKì´ë…¸ë² ì´ì…˜", "ì…€íŠ¸ë¦¬ì˜¨", "ì‚¼ì„±ë¬¼ì‚°",
                "ì‹ í•œì§€ì£¼", "í˜„ëŒ€ëª¨ë¹„ìŠ¤", "ì¹´ì¹´ì˜¤ë±…í¬", "SK", "LGì „ì",
                "í•œêµ­ì „ë ¥", "S-Oil", "í•˜ë‚˜ê¸ˆìœµì§€ì£¼", "í¬ë˜í”„í†¤", "ì‚¼ì„±ìƒëª…",
                "LG", "SKí…”ë ˆì½¤", "KT&G", "ì¹´ì¹´ì˜¤í˜ì´", "ì‚¼ì„±ì „ê¸°",
                "ì‚¼ì„±ì—ìŠ¤ë””ì—ìŠ¤", "ìš°ë¦¬ê¸ˆìœµì§€ì£¼", "ê³ ë ¤ì•„ì—°", "LGìƒí™œê±´ê°•", "í¬ìŠ¤ì½”ì¼€ì´ì¹¼",
                "ì—”ì”¨ì†Œí”„íŠ¸", "KT", "ì‚¼ì„±í™”ì¬", "SKë°”ì´ì˜¤ì‚¬ì´ì–¸ìŠ¤", "í•˜ì´ë¸Œ",
                "ì•„ëª¨ë ˆí¼ì‹œí”½", "ê¸°ì—…ì€í–‰", "SKì•„ì´ì´í…Œí¬ë†€ë¡œì§€", "í˜„ëŒ€ê¸€ë¡œë¹„ìŠ¤", "ë¡¯ë°ì¼€ë¯¸ì¹¼",
                "ë„·ë§ˆë¸”", "í•œêµ­ì¡°ì„ í•´ì–‘", "SKë°”ì´ì˜¤íŒœ", "LGë””ìŠ¤í”Œë ˆì´", "í•œì˜¨ì‹œìŠ¤í…œ"
            ]
            
            pages = int(input("\ní¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 25): ") or "25")
            headless = input("í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ? (y/N): ").lower() in ['y', 'yes']
            
            print(f"\nâš ï¸ ì£¼ì˜ì‚¬í•­:")
            print(f"â€¢ 50ê°œ ê¸°ì—… í¬ë¡¤ë§ì€ ìƒë‹¹í•œ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤ (ì˜ˆìƒ: 3-5ì‹œê°„)")
            if use_ai_classification:
                estimated_cost = len(top50_companies) * pages * 0.1  # ë°°ì¹˜ ìµœì í™”ë¡œ ëŒ€í­ ì ˆì•½
                print(f"â€¢ ë°°ì¹˜ ìµœì í™”ë¡œ OpenAI API ë¹„ìš© ì ˆì•½ (ì˜ˆìƒ: ${estimated_cost:.2f}, ê¸°ì¡´ ëŒ€ë¹„ 90% ì ˆì•½)")
            print(f"â€¢ ê° ê¸°ì—…ê°„ 30ì´ˆì”© ëŒ€ê¸°í•˜ì—¬ ì„œë²„ ë¶€í•˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤")
            print(f"â€¢ ì¤‘ê°„ì— Ctrl+Cë¡œ ì¤‘ë‹¨ ê°€ëŠ¥í•˜ë©°, ê·¸ë•Œê¹Œì§€ì˜ ê²°ê³¼ëŠ” ì €ì¥ë©ë‹ˆë‹¤")
            
            confirm = input("\nê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower()
            if confirm in ['y', 'yes']:
                run_multiple_companies_crawl(
                    company_list=top50_companies,
                    pages=pages, 
                    headless=headless,
                    delay_between_companies=30,
                    use_ai_classification=use_ai_classification,
                    openai_api_key=openai_api_key,
                    enable_spell_check=enable_spell_check
                )
            else:
                print("âŒ í¬ë¡¤ë§ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()