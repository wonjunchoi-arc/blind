"""
ë¸”ë¼ì¸ë“œ í¬ë¡¤ë§ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (RAG ìµœì í™” ì§€ì›)

ê°„í¸í•œ ì‹¤í–‰ ë„êµ¬:
- ì„¤ì • íŒŒì¼ ê¸°ë°˜ ìë™ í¬ë¡¤ë§
- RAG ìµœì í™” JSON ìë™ ìƒì„±
- íšŒì‚¬ë³„ ë§ì¶¤ ì„¤ì • ì ìš©
- ì§„í–‰ ìƒí™© ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- ì—ëŸ¬ ë°œìƒ ì‹œ ìë™ ë³µêµ¬

ì‚¬ìš©ë²•:
python run_crawler.py --company naver --pages 10
python run_crawler.py --company kakao --output custom_filename.xlsx --rag
python run_crawler.py --company samsung --pages 50 --headless --no-rag
python run_crawler.py --help  # ë„ì›€ë§ ë³´ê¸°
"""
# run_crawler.py

import argparse
import sys
import os
from pathlib import Path

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

try:
    from blind_review_crawler import BlindReviewCrawler
    from config import (
        get_company_config, 
        ensure_directories, 
        get_output_filepath,
        validate_config,
        get_rag_settings,
        is_rag_enabled,
        print_config_summary,
        CRAWLING_SETTINGS,
        OUTPUT_SETTINGS,
        RAG_SETTINGS
    )
except ImportError as e:
    print(f"í•„ìš”í•œ ëª¨ë“ˆì„ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    print("ë‹¤ìŒ íŒŒì¼ë“¤ì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”:")
    print("- blind_review_crawler.py")
    print("- config.py")
    sys.exit(1)

import logging

def setup_logging(verbose=False):
    """ë¡œê¹… ì„¤ì •"""
    log_level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)

def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹± (RAG ì˜µì…˜ ì¶”ê°€)"""
    parser = argparse.ArgumentParser(
        description="ë¸”ë¼ì¸ë“œ ê¸°ì—… ë¦¬ë·° í¬ë¡¤ë§ ë„êµ¬ (RAG ìµœì í™” ì§€ì›)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python run_crawler.py --company naver --pages 10
  python run_crawler.py --company kakao --output kakao_reviews.xlsx --rag
  python run_crawler.py --company samsung --pages 50 --headless --no-rag

RAG ê´€ë ¨ ì˜µì…˜:
  --rag            RAG ìµœì í™” JSON ìƒì„± (ê¸°ë³¸ê°’: config ì„¤ì • ë”°ë¦„)
  --no-rag         RAG ê¸°ëŠ¥ ë¹„í™œì„±í™”
  --rag-only       RAG JSONë§Œ ìƒì„± (Excel ìƒëµ)

ì§€ì›í•˜ëŠ” íšŒì‚¬:
  - naver: ë„¤ì´ë²„
  - kakao: ì¹´ì¹´ì˜¤  
  - samsung: ì‚¼ì„±ì „ì
  - lg: LGì „ì
  
ì£¼ì˜ì‚¬í•­:
  - í¬ë¡¤ë§ ì „ ë¸”ë¼ì¸ë“œ ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤
  - ê³¼ë„í•œ ìš”ì²­ìœ¼ë¡œ ì¸í•œ IP ì°¨ë‹¨ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì ì ˆí•œ ê°„ê²©ì„ ë‘ê³  ì‹¤í–‰í•˜ì„¸ìš”
  - RAG ìµœì í™” ì‹œ ì²˜ë¦¬ ì‹œê°„ì´ ì¦ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        """
    )
    
    # ê¸°ë³¸ ì¸ìë“¤
    parser.add_argument(
        "--company", 
        type=str, 
        required=True,
        help="í¬ë¡¤ë§í•  íšŒì‚¬ (ì˜ˆ: naver, kakao, samsung, lg)"
    )
    
    parser.add_argument(
        "--pages",
        type=int,
        help="í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: ì„¤ì • íŒŒì¼ì˜ ê°’)"
    )
    
    parser.add_argument(
        "--output",
        type=str,
        help="ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸ê°’: ì„¤ì • íŒŒì¼ì˜ ê°’)"
    )
    
    parser.add_argument(
        "--headless",
        action="store_true",
        help="í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ì‹¤í–‰ (ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¹€)"
    )
    
    parser.add_argument(
        "--timeout",
        type=int,
        default=CRAWLING_SETTINGS["wait_timeout"],
        help=f"ìš”ì†Œ ëŒ€ê¸° ì‹œê°„ (ì´ˆ, ê¸°ë³¸ê°’: {CRAWLING_SETTINGS['wait_timeout']})"
    )
    
    parser.add_argument(
        "--delay",
        type=float,
        default=CRAWLING_SETTINGS["between_pages_delay"],
        help=f"í˜ì´ì§€ ê°„ ê°„ê²© (ì´ˆ, ê¸°ë³¸ê°’: {CRAWLING_SETTINGS['between_pages_delay']})"
    )
    
    # RAG ê´€ë ¨ ì¸ìë“¤ (ìƒˆë¡œ ì¶”ê°€)
    rag_group = parser.add_mutually_exclusive_group()
    rag_group.add_argument(
        "--rag",
        action="store_true",
        help="RAG ìµœì í™” JSON ìƒì„± í™œì„±í™”"
    )
    
    rag_group.add_argument(
        "--no-rag",
        action="store_true", 
        help="RAG ê¸°ëŠ¥ ì™„ì „ ë¹„í™œì„±í™”"
    )
    
    parser.add_argument(
        "--rag-only",
        action="store_true",
        help="RAG JSONë§Œ ìƒì„± (ê¸°ë³¸ Excel íŒŒì¼ ìƒëµ)"
    )
    
    # ê¸°íƒ€ ì˜µì…˜ë“¤
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true", 
        help="ì‹¤ì œ í¬ë¡¤ë§ ì—†ì´ ì„¤ì •ë§Œ í™•ì¸"
    )
    
    parser.add_argument(
        "--config-summary",
        action="store_true",
        help="ì„¤ì • ìš”ì•½ ì¶œë ¥ í›„ ì¢…ë£Œ"
    )
    
    return parser.parse_args()

def determine_rag_settings(args):
    """RAG ì„¤ì • ê²°ì •"""
    # ëª…ë ¹í–‰ ì¸ìê°€ ìš°ì„ ìˆœìœ„ ìµœê³ 
    if args.no_rag:
        return {
            "enable_rag": False,
            "excel_output": True,
            "reason": "ëª…ë ¹í–‰ì—ì„œ --no-rag ì§€ì •"
        }
    
    if args.rag:
        return {
            "enable_rag": True,
            "excel_output": not args.rag_only,
            "reason": "ëª…ë ¹í–‰ì—ì„œ --rag ì§€ì •"
        }
    
    if args.rag_only:
        return {
            "enable_rag": True,
            "excel_output": False,
            "reason": "ëª…ë ¹í–‰ì—ì„œ --rag-only ì§€ì •"
        }
    
    # ì„¤ì • íŒŒì¼ ê¸°ë³¸ê°’ ì‚¬ìš©
    return {
        "enable_rag": is_rag_enabled(),
        "excel_output": True,
        "reason": "config.py ì„¤ì • ì‚¬ìš©"
    }

def validate_arguments(args, logger):
    """ì¸ì ìœ íš¨ì„± ê²€ì‚¬ (RAG ê²€ì‚¬ ì¶”ê°€)"""
    try:
        # íšŒì‚¬ ì„¤ì • í™•ì¸
        company_config = get_company_config(args.company)
        logger.info(f"íšŒì‚¬ ì„¤ì • í™•ì¸: {company_config['name']}")
        
        # í˜ì´ì§€ ìˆ˜ í™•ì¸
        pages = args.pages if args.pages else company_config["max_pages"]
        if pages <= 0 or pages > 1000:  # ì•ˆì „ì„ ìœ„í•œ ìƒí•œì„ 
            raise ValueError(f"í˜ì´ì§€ ìˆ˜ëŠ” 1-1000 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤: {pages}")
        
        # íƒ€ì„ì•„ì›ƒ í™•ì¸
        if args.timeout <= 0 or args.timeout > 60:
            raise ValueError(f"íƒ€ì„ì•„ì›ƒì€ 1-60ì´ˆ ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤: {args.timeout}")
        
        # ì§€ì—°ì‹œê°„ í™•ì¸  
        if args.delay < 0 or args.delay > 60:
            raise ValueError(f"ì§€ì—°ì‹œê°„ì€ 0-60ì´ˆ ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤: {args.delay}")
        
        # RAG ì„¤ì • í™•ì¸
        rag_settings = determine_rag_settings(args)
        logger.info(f"RAG ì„¤ì •: {rag_settings['reason']}")
        
        return company_config, pages, rag_settings
        
    except KeyError as e:
        from config import COMPANIES
        available_companies = list(COMPANIES.keys())
        logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íšŒì‚¬: {args.company}")
        logger.error(f"ì‚¬ìš© ê°€ëŠ¥í•œ íšŒì‚¬: {', '.join(available_companies)}")
        raise
    except ValueError as e:
        logger.error(f"ì˜ëª»ëœ ì¸ì: {e}")
        raise

def print_banner():
    """ì‹œì‘ ë°°ë„ˆ ì¶œë ¥ (RAG ì§€ì› í‘œì‹œ)"""
    banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   ğŸ” ë¸”ë¼ì¸ë“œ í¬ë¡¤ë§ ë„êµ¬ v2.0                  â•‘
â•‘                        (RAG ìµœì í™” ì§€ì›)                       â•‘
â•‘                                                              â•‘
â•‘  ğŸ“Š ê¸°ì—… ë¦¬ë·° ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤           â•‘
â•‘  ğŸ¤– RAG ì‹œìŠ¤í…œì„ ìœ„í•œ ìµœì í™”ëœ JSON ìƒì„± ê¸°ëŠ¥                  â•‘
â•‘  âš ï¸  ì‚¬ìš© ì „ ë¸”ë¼ì¸ë“œ ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print(banner)

def print_summary(company_config, pages, output_files, rag_settings, headless):
    """í¬ë¡¤ë§ ì„¤ì • ìš”ì•½ ì¶œë ¥ (RAG ì •ë³´ í¬í•¨)"""
    print("\ní¬ë¡¤ë§ ì„¤ì • ìš”ì•½:")
    print("â”€" * 60)
    print(f"íšŒì‚¬ëª…: {company_config['name']}")
    print(f"URL: {company_config['url']}")
    print(f"í˜ì´ì§€ ìˆ˜: {pages}")
    print(f"ë¸Œë¼ìš°ì €: {'í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ' if headless else 'ì¼ë°˜ ëª¨ë“œ'}")
    print(f"RAG ìµœì í™”: {'í™œì„±í™”' if rag_settings['enable_rag'] else 'ë¹„í™œì„±í™”'}")
    
    # ì¶œë ¥ íŒŒì¼ ì •ë³´
    print("\nìƒì„±ë  íŒŒì¼:")
    if rag_settings['excel_output']:
        print(f"  - Excel: {output_files.get('excel', 'N/A')}")
    if rag_settings['enable_rag']:
        print(f"  - RAG JSON: {output_files.get('rag_json', 'N/A')}")
    
    if rag_settings['enable_rag']:
        rag_config = get_rag_settings()
        print(f"\nRAG ê¸°ëŠ¥:")
        print(f"  - í‚¤ì›Œë“œ ì¶”ì¶œ: {'í™œì„±í™”' if rag_config['keyword_extraction']['enable'] else 'ë¹„í™œì„±í™”'}")
        print(f"  - ê°ì„± ë¶„ì„: {'í™œì„±í™”' if rag_config['sentiment_analysis']['enable'] else 'ë¹„í™œì„±í™”'}")
        print(f"  - ì˜ë¯¸ì  ì½˜í…ì¸ : {'ìƒì„±' if rag_config['content_generation']['semantic_content'] else 'ë¯¸ìƒì„±'}")
        print(f"  - ì¸í…”ë¦¬ì „ìŠ¤ í”Œë˜ê·¸: {'í™œì„±í™”' if rag_config['metadata_enrichment']['intelligence_flags'] else 'ë¹„í™œì„±í™”'}")
    
    print("â”€" * 60)

def create_enhanced_crawler(args, rag_settings):
    """í–¥ìƒëœ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    
    # ê¸°ë³¸ í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = BlindReviewCrawler(
        headless=args.headless,
        wait_timeout=args.timeout
    )
    
    # RAG ì„¤ì •ì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ê¸°ë³¸ í¬ë¡¤ëŸ¬ ë°˜í™˜
    if not rag_settings['enable_rag']:
        # RAG í”„ë¡œì„¸ì„œ ë¹„í™œì„±í™”
        crawler.rag_processor = None
    
    return crawler

def execute_crawling(crawler, company_config, pages, company_key, rag_settings, output_files):
    """í¬ë¡¤ë§ ì‹¤í–‰ ë° ê²°ê³¼ ì €ì¥"""
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    crawler.crawl_reviews(
        base_url=company_config["url"],
        last_page=pages,
        company_name=company_key
    )
    
    return True

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë°°ë„ˆ ì¶œë ¥
    print_banner()
    
    # ë¡œê¹… ì„¤ì •
    logger = setup_logging()
    
    try:
        # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
        args = parse_arguments()
        
        # ì„¤ì • ìš”ì•½ë§Œ ì¶œë ¥í•˜ê³  ì¢…ë£Œ
        if args.config_summary:
            print_config_summary()
            return
        
        # ìƒì„¸ ë¡œê¹… ì„¤ì •
        if args.verbose:
            logging.getLogger().setLevel(logging.DEBUG)
            logger.debug("ìƒì„¸ ë¡œê¹… ëª¨ë“œ í™œì„±í™”")
        
        # ì„¤ì • ê²€ì¦
        logger.info("ì„¤ì • ê²€ì¦ ì¤‘...")
        validate_config()
        ensure_directories()
        
        # ì¸ì ìœ íš¨ì„± ê²€ì‚¬
        company_config, pages, rag_settings = validate_arguments(args, logger)
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •
        output_files = {}
        
        if rag_settings['excel_output']:
            output_files['excel'] = get_output_filepath(
                args.company, 
                file_type="basic",
                custom_filename=args.output
            )
        
        if rag_settings['enable_rag']:
            output_files['rag_json'] = get_output_filepath(
                args.company,
                file_type="rag"
            )
        
        # ì„¤ì • ìš”ì•½ ì¶œë ¥
        print_summary(company_config, pages, output_files, rag_settings, args.headless)
        
        # Dry run ëª¨ë“œì¸ ê²½ìš° ì—¬ê¸°ì„œ ì¢…ë£Œ
        if args.dry_run:
            logger.info("Dry run ì™„ë£Œ - ì‹¤ì œ í¬ë¡¤ë§ì€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            print(f"\nRAG ì„¤ì • ìƒì„¸:")
            print(f"  - í™œì„±í™” ì´ìœ : {rag_settings['reason']}")
            if rag_settings['enable_rag']:
                rag_config = get_rag_settings()
                print(f"  - í‚¤ì›Œë“œ ë§¥ë½ ê¸¸ì´: {rag_config['keyword_extraction']['context_length']}")
                print(f"  - ê°ì„± ë¶„ì„ ë°©ë²•: {rag_config['sentiment_analysis']['method']}")
            return
        
        # ì‚¬ìš©ì í™•ì¸
        if not args.headless:
            response = input("\nê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if response not in ['y', 'yes', 'ì˜ˆ']:
                logger.info("ì‚¬ìš©ìê°€ í¬ë¡¤ë§ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤")
                return
        
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        logger.info("í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì¤‘...")
        crawler = create_enhanced_crawler(args, rag_settings)
        
        # í˜ì´ì§€ ê°„ ê°„ê²© ì„¤ì • (ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸)
        if hasattr(crawler, 'between_pages_delay'):
            crawler.between_pages_delay = args.delay
        
        logger.info("í¬ë¡¤ë§ ì‹œì‘...")
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        success = execute_crawling(
            crawler, company_config, pages, args.company, 
            rag_settings, output_files
        )
        
        if success:
            logger.info("í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"\ní¬ë¡¤ë§ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            # ìƒì„±ëœ íŒŒì¼ ì •ë³´ ì¶œë ¥
            print(f"\nìƒì„±ëœ íŒŒì¼:")
            if rag_settings['excel_output'] and 'excel' in output_files:
                print(f"  - ê¸°ë³¸ Excel: {output_files['excel']}")
                
            if rag_settings['enable_rag'] and 'rag_json' in output_files:
                print(f"  - RAG JSON: {output_files['rag_json']}")
                print(f"    * ë²¡í„° ê²€ìƒ‰ì— ìµœì í™”ëœ êµ¬ì¡°")
                print(f"    * ì²­í‚¹ì— ì í•©í•œ ë°ì´í„° í˜•íƒœ")
                print(f"    * í‚¤ì›Œë“œ ë° ê°ì„± ë¶„ì„ í¬í•¨")
            
            # RAG í™œìš© ê°€ì´ë“œ
            if rag_settings['enable_rag']:
                print(f"\nRAG ì‹œìŠ¤í…œ í™œìš© ê°€ì´ë“œ:")
                print(f"  1. JSON íŒŒì¼ì„ ë²¡í„° DBì— ì§ì ‘ ë¡œë“œ ê°€ëŠ¥")
                print(f"  2. content_semantic: ë²¡í„° ê²€ìƒ‰ìš©")
                print(f"  3. content_keyword: í‚¤ì›Œë“œ ê²€ìƒ‰ìš©") 
                print(f"  4. extracted_keywords: ì¶”ì¶œëœ í‚¤ì›Œë“œ ëª©ë¡")
                print(f"  5. intelligence_flags: ê³ ê¸‰ ë¶„ì„ í”Œë˜ê·¸")
        
    except KeyboardInterrupt:
        logger.warning("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        sys.exit(1)
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if args.verbose:
            import traceback
            logger.error(traceback.format_exc())
        sys.exit(1)
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        if 'crawler' in locals():
            crawler.close()
            logger.info("ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

if __name__ == "__main__":
    main()