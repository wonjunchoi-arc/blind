# text_processor.py
"""
í…ìŠ¤íŠ¸ ì •ê·œí™” ë° OpenAI ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ë¶„ë¥˜ ëª¨ë“ˆ v3.2
- ë°°ì¹˜ í¬ê¸° ìµœëŒ€ 50ê°œë¡œ í™•ëŒ€
- API í˜¸ì¶œ ìµœì í™” ë° ë¹„ìš© ì ˆì•½
- ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì„±ëŠ¥ í–¥ìƒ
- ê°œì„ ëœ ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°± ë©”ì»¤ë‹ˆì¦˜

ì„¤ì¹˜ í•„ìš”í•œ íŒ¨í‚¤ì§€:
pip install hanspell openai python-dotenv tqdm
"""

import re
import json
import time
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import os
from dotenv import load_dotenv
from tqdm import tqdm

try:
    from hanspell import spell_checker
except ImportError:
    spell_checker = None

try:
    import openai
    from openai import OpenAI
except ImportError:
    openai = None

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì • ê°„ì†Œí™”
logging.basicConfig(level=logging.ERROR)
logger = logging.getLogger(__name__)

@dataclass
class CategoryResult:
    """ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ê²°ê³¼"""
    primary_category: str
    primary_confidence: float
    secondary_category: str = ""
    secondary_confidence: float = 0.0

class TextNormalizer:
    """í…ìŠ¤íŠ¸ ì •ê·œí™” ì²˜ë¦¬ê¸°"""
    
    def __init__(self, enable_spell_check=True):
        self.enable_spell_check = enable_spell_check and spell_checker is not None
        
        # ì •ê·œí™” íŒ¨í„´
        self.patterns = {
            'html_tags': re.compile(r'<[^>]+>'),
            'special_chars': re.compile(r'[^\w\sã„±-ã…ã…-ã…£ê°€-í£.,!?()-]'),
            'multiple_spaces': re.compile(r'\s+'),
            'multiple_punct': re.compile(r'([.!?]){2,}'),
            'unnecessary_symbols': re.compile(r'[ã„±-ã…ã…-ã…£]+')
        }
    
    def normalize_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        if not text or not isinstance(text, str):
            return ""
        
        try:
            # ê¸°ë³¸ ì •ë¦¬
            normalized = self._basic_cleanup(text)
            
            # ë§ì¶¤ë²• ê²€ì‚¬ (100ì ì´í•˜ë§Œ, ì¡°ìš©í•˜ê²Œ ì²˜ë¦¬)
            if self.enable_spell_check and 5 < len(normalized) < 200:
                try:
                    result = spell_checker.check(normalized)
                    normalized = result.checked
                except:
                    pass  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ì‚¬ìš©
            
            return self._final_cleanup(normalized)
            
        except Exception:
            return self._basic_cleanup(text)
    
    def _basic_cleanup(self, text: str) -> str:
        """ê¸°ë³¸ ì •ë¦¬"""
        text = self.patterns['html_tags'].sub('', text)
        text = self.patterns['unnecessary_symbols'].sub('', text)
        text = self.patterns['special_chars'].sub(' ', text)
        text = self.patterns['multiple_punct'].sub(r'\1', text)
        text = self.patterns['multiple_spaces'].sub(' ', text)
        return text
    
    def _final_cleanup(self, text: str) -> str:
        """ìµœì¢… ì •ë¦¬"""
        text = self.patterns['multiple_spaces'].sub(' ', text)
        text = text.strip()
        text = re.sub(r'\(\s*\)', '', text)
        text = re.sub(r'\[\s*\]', '', text)
        return text

class OptimizedBatchClassifier:
    """ìµœì í™”ëœ OpenAI ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ë¶„ë¥˜ê¸°"""
    
    def __init__(self, api_key: str = None, model: str = "gpt-4o-mini-2024-07-18"):
        if not openai:
            raise ImportError("openai ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        self.client = OpenAI(api_key=self.api_key)
        self.model = model
        
        # 5ê°œ ì¹´í…Œê³ ë¦¬ ì •ì˜
        self.categories = {
            "career_growth": "ì»¤ë¦¬ì–´ í–¥ìƒ",
            "salary_benefits": "ê¸‰ì—¬ ë³µì§€",  
            "work_life_balance": "ì›Œë¼ë°¸",
            "company_culture": "ì‚¬ë‚´ë¬¸í™”",
            "management": "ê²½ì˜ì§„"
        }
        
        # ìµœì í™”ëœ ë°°ì¹˜ ì„¤ì •
        self.default_batch_size = 30  # ëŒ€ìš©ëŸ‰ ë°°ì¹˜ í¬ê¸°
        self.max_retries = 3
        self.retry_delay = 2
        
        # í†µê³„ ì¶”ì 
        self.total_api_calls = 0
        self.total_tokens_used = 0
        self.successful_batches = 0
        self.failed_batches = 0
    
    def classify_chunks_batch(self, chunks: List[str], batch_size: int = None) -> List[CategoryResult]:
        """ëŒ€ìš©ëŸ‰ ì²­í¬ ë°°ì¹˜ ë¶„ë¥˜"""
        if not chunks:
            return []
        
        # ë°°ì¹˜ í¬ê¸° ì„¤ì •
        effective_batch_size = batch_size or self.default_batch_size
        
        all_results = []
        total_batches = (len(chunks) + effective_batch_size - 1) // effective_batch_size
        
        print(f"ğŸ” ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ë¶„ë¥˜ ì‹œì‘:")
        print(f"   - ì´ ì²­í¬: {len(chunks)}ê°œ")
        print(f"   - ë°°ì¹˜ í¬ê¸°: {effective_batch_size}ê°œ")
        print(f"   - ì˜ˆìƒ ë°°ì¹˜ ìˆ˜: {total_batches}ê°œ")
        
        for i in range(0, len(chunks), effective_batch_size):
            batch = chunks[i:i + effective_batch_size]
            current_batch_num = i // effective_batch_size + 1
            
            try:
                batch_results = self._process_large_batch(batch, current_batch_num, total_batches)
                all_results.extend(batch_results)
                self.successful_batches += 1
                
            except Exception as e:
                print(f"\nâš ï¸ ë°°ì¹˜ {current_batch_num} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)[:100]}...")
                # í´ë°± ì²˜ë¦¬
                fallback_results = [self._create_fallback_result() for _ in batch]
                all_results.extend(fallback_results)
                self.failed_batches += 1
            
            # API ë ˆì´íŠ¸ ì œí•œ ê³ ë ¤
            if i + effective_batch_size < len(chunks):
                print(f"â³ API ë ˆì´íŠ¸ ì œí•œìœ¼ë¡œ 1ì´ˆ ëŒ€ê¸°...")
                time.sleep(1)
        
        print(f"\nğŸ“Š ë°°ì¹˜ ë¶„ë¥˜ ì™„ë£Œ:")
        print(f"   - ì„±ê³µí•œ ë°°ì¹˜: {self.successful_batches}ê°œ")
        print(f"   - ì‹¤íŒ¨í•œ ë°°ì¹˜: {self.failed_batches}ê°œ")
        print(f"   - ì´ API í˜¸ì¶œ: {self.total_api_calls}íšŒ")
        
        return all_results
    
    def _process_large_batch(self, batch: List[str], batch_num: int, total_batches: int) -> List[CategoryResult]:
        """ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ë‹¨ì¼ ì²˜ë¦¬"""
        
        print(f"\nğŸ” ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì‹œì‘ ({len(batch)}ê°œ ì²­í¬)")
        batch_start_time = time.time()
        
        for attempt in range(self.max_retries):
            try:
                # 1ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„±
                print(f"  ğŸ“ 1ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
                start_time = time.time()
                
                prompt = self._create_optimized_batch_prompt(batch)
                prompt_time = time.time() - start_time
                print(f"  âœ… í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ ({prompt_time:.3f}s)")
                
                # 2ë‹¨ê³„: API í˜¸ì¶œ (ê°€ì¥ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ë¶€ë¶„)
                print(f"  ğŸ¤– 2ë‹¨ê³„: OpenAI API í˜¸ì¶œ ì¤‘... (ëª¨ë¸: {self.model})")
                api_start_time = time.time()
                
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": self._get_optimized_system_prompt()},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.1,
                    max_tokens=5000,
                    timeout=60  # íƒ€ì„ì•„ì›ƒ ì„¤ì •
                )
                
                api_time = time.time() - api_start_time
                print(f"  âœ… API í˜¸ì¶œ ì™„ë£Œ ({api_time:.1f}s) - í† í°: {response.usage.total_tokens}")
                
                self.total_api_calls += 1
                self.total_tokens_used += response.usage.total_tokens
                
                # 3ë‹¨ê³„: ì‘ë‹µ íŒŒì‹±
                print(f"  ğŸ”§ 3ë‹¨ê³„: ì‘ë‹µ íŒŒì‹± ì¤‘...")
                parse_start_time = time.time()
                
                result_text = response.choices[0].message.content.strip()
                results = self._parse_optimized_batch_response(result_text, len(batch))
                
                parse_time = time.time() - parse_start_time
                print(f"  âœ… ì‘ë‹µ íŒŒì‹± ì™„ë£Œ ({parse_time:.3f}s) - ê²°ê³¼: {len(results)}ê°œ")
                
                # 4ë‹¨ê³„: ê²°ê³¼ ê²€ì¦
                print(f"  âœ”ï¸ 4ë‹¨ê³„: ê²°ê³¼ ê²€ì¦ ì¤‘...")
                validation_start_time = time.time()
                
                # ê²°ê³¼ ê²€ì¦
                if len(results) == len(batch):
                    validation_time = time.time() - validation_start_time
                    batch_total_time = time.time() - batch_start_time
                    
                    print(f"  âœ… ê²°ê³¼ ê²€ì¦ ì™„ë£Œ ({validation_time:.3f}s)")
                    print(f"ğŸ‰ ë°°ì¹˜ {batch_num} ì™„ë£Œ! ì´ ì‹œê°„: {batch_total_time:.1f}s")
                    
                    # ì²« ë°°ì¹˜ë§Œ ìƒì„¸ ë¶„ì„ ì¶œë ¥
                    if batch_num == 1:
                        print(f"\nğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ë¶„ì„:")
                        print(f"   - í”„ë¡¬í”„íŠ¸ ìƒì„±: {prompt_time:.3f}s ({prompt_time/batch_total_time*100:.1f}%)")
                        print(f"   - API í˜¸ì¶œ: {api_time:.1f}s ({api_time/batch_total_time*100:.1f}%) âš ï¸ [ì£¼ìš” ë³‘ëª©]")
                        print(f"   - ì‘ë‹µ íŒŒì‹±: {parse_time:.3f}s ({parse_time/batch_total_time*100:.1f}%)")
                        print(f"   - ê²°ê³¼ ê²€ì¦: {validation_time:.3f}s ({validation_time/batch_total_time*100:.1f}%)")
                        print(f"   - ì´ ì‹œê°„: {batch_total_time:.1f}s")
                    
                    return results
                else:
                    # ë¶€ì¡±í•œ ê²°ê³¼ëŠ” í´ë°±ìœ¼ë¡œ ì±„ì›€
                    print(f"  âš ï¸ ê²°ê³¼ ë¶€ì¡± ({len(results)}/{len(batch)}) - í´ë°±ìœ¼ë¡œ ë³´ì •")
                    while len(results) < len(batch):
                        results.append(self._create_fallback_result())
                    
                    validation_time = time.time() - validation_start_time
                    batch_total_time = time.time() - batch_start_time
                    
                    print(f"  âœ… í´ë°± ë³´ì • ì™„ë£Œ ({validation_time:.3f}s)")
                    print(f"ğŸ‰ ë°°ì¹˜ {batch_num} ì™„ë£Œ! ì´ ì‹œê°„: {batch_total_time:.1f}s")
                    
                    return results[:len(batch)]
                
            except Exception as e:
                if attempt < self.max_retries - 1:
                    print(f"\nâŒ ë°°ì¹˜ {batch_num} ì‹œë„ {attempt+1} ì‹¤íŒ¨: {str(e)[:80]}...")
                    retry_delay = self.retry_delay * (attempt + 1)
                    print(f"â³ {retry_delay}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(retry_delay)
                else:
                    print(f"\nğŸ’¥ ë°°ì¹˜ {batch_num} ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ - í´ë°± ì²˜ë¦¬")
                    raise e
        
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ì‹œ í´ë°±
        return [self._create_fallback_result() for _ in batch]
    
    def _get_optimized_system_prompt(self) -> str:
        """ìµœì í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        return 
    """ë‹¹ì‹ ì€ ë¸”ë¼ì¸ë“œ(Blind) ê¸°ì—… ë¦¬ë·° ë¶„ë¥˜ ì „ë¬¸ AIì…ë‹ˆë‹¤.  
ì£¼ì–´ì§„ ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ì•„ë˜ 5ê°œ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë¶„ë¥˜í•˜ì„¸ìš”.  

ì¹´í…Œê³ ë¦¬ ì •ì˜:  
1. career_growth â†’ ìŠ¹ì§„, êµìœ¡, ì„±ì¥, ê°œë°œ, ì»¤ë¦¬ì–´ ê¸°íšŒ, ìŠ¤í‚¬ í–¥ìƒ ê´€ë ¨  
2. salary_benefits â†’ ê¸‰ì—¬, ì—°ë´‰, ë³µì§€, ë³´ë„ˆìŠ¤, ì¸ì„¼í‹°ë¸Œ, íœ´ê°€, ë³´í—˜, ë³µë¦¬í›„ìƒ ê´€ë ¨  
3. work_life_balance â†’ ê·¼ë¬´ì‹œê°„, ì•¼ê·¼, ì›Œë¼ë°¸, íœ´ì¼, ìŠ¤íŠ¸ë ˆìŠ¤, í”¼ë¡œ ê´€ë ¨  
4. company_culture â†’ íšŒì‚¬ ë¶„ìœ„ê¸°, ì¡°ì§ë¬¸í™”, ë™ë£Œ ê´€ê³„, ì¸ê°„ê´€ê³„, ì†Œí†µ, í˜‘ì—… ê´€ë ¨  
5. management â†’ ê²½ì˜ì§„, ìƒì‚¬, ê´€ë¦¬ì, ë¦¬ë”ì‹­, ì˜ì‚¬ê²°ì •, ë°©í–¥ì„±, ë¹„ì „ ê´€ë ¨  

ë¶„ë¥˜ ê·œì¹™:  
- ê° ë¦¬ë·°ë‹¹ ë°˜ë“œì‹œ 1ê°œì˜ primary_categoryë§Œ ì„ íƒ  
- primary_confidenceëŠ” 0.1~1.0 ë²”ìœ„ì˜ ì†Œìˆ˜ì  ìˆ˜ì¹˜  
  - í™•ì‹¤í•œ ê²½ìš°: 0.9 ì´ìƒ  
  - ì¤‘ê°„ ì •ë„ í™•ì‹ : 0.6 ~ 0.8  
  - ì• ë§¤í•œ ê²½ìš°: 0.5 ì´í•˜  
- ë‚´ë¶€ì ìœ¼ë¡œ ë‹¨ê³„ì ìœ¼ë¡œ ìƒê°í•´ ê·¼ê±°ë¥¼ íŒë‹¨í•œ ë’¤, ìµœì¢… ì‘ë‹µì€ **JSON ë°°ì—´ë§Œ ì¶œë ¥**  
- ì…ë ¥ í…ìŠ¤íŠ¸ ìˆœì„œì™€ ì¶œë ¥ ìˆœì„œ ë°˜ë“œì‹œ ë™ì¼  

ì‘ë‹µ í˜•ì‹ (ì˜ˆì‹œ):  
[{"primary_category": "salary_benefits", "primary_confidence": 0.85}]

---

### Few-shot ì˜ˆì‹œ

ì…ë ¥:  
["ì—°ë´‰ì€ ì—…ê³„ í‰ê·  ì´ìƒì´ê³  ë³µì§€ ì œë„ë„ ì˜ ë˜ì–´ ìˆë‹¤."]  

ì¶œë ¥:  
[{"primary_category": "salary_benefits", "primary_confidence": 0.95}]  

ì…ë ¥:  
["ì—…ë¬´ ê°•ë„ê°€ ë†’ì•„ì„œ ì•¼ê·¼ì´ ë§ê³  ì›Œë¼ë°¸ì´ ê±°ì˜ ì—†ë‹¤."]  

ì¶œë ¥:  
[{"primary_category": "work_life_balance", "primary_confidence": 0.92}]  

---

ì´ì œ ì‹¤ì œ ì…ë ¥ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ë¼.
"""
# """ë‹¹ì‹ ì€ í•œêµ­ ê¸°ì—… ë¦¬ë·° ë¶„ì„ ì „ë¬¸ AIì…ë‹ˆë‹¤. 
# ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë“¤ì„ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ë‹¤ìŒ 5ê°œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:

# 1. career_growth: ìŠ¹ì§„/êµìœ¡/ì„±ì¥/ê°œë°œ/ì»¤ë¦¬ì–´/ìŠ¤í‚¬
# 2. salary_benefits: ê¸‰ì—¬/ì—°ë´‰/ë³µì§€/ë³´ë„ˆìŠ¤/ì¸ì„¼í‹°ë¸Œ/íœ´ê°€/ë³´í—˜
# 3. work_life_balance: ì•¼ê·¼/ì›Œë¼ë°¸/ê·¼ë¬´ì‹œê°„/íœ´ì¼/ìŠ¤íŠ¸ë ˆìŠ¤/í”¼ë¡œ
# 4. company_culture: ë¶„ìœ„ê¸°/ë¬¸í™”/ë™ë£Œ/ì¸ê°„ê´€ê³„/ì†Œí†µ/ì¡°ì§
# 5. management: ê²½ì˜ì§„/ìƒì‚¬/ë¦¬ë”ì‹­/ì˜ì‚¬ê²°ì •/ë°©í–¥ì„±/ë¹„ì „

# ì‘ë‹µ ê·œì¹™:
# - ê° í…ìŠ¤íŠ¸ë§ˆë‹¤ 1ìˆœìœ„ ì¹´í…Œê³ ë¦¬ ì„ íƒ
# - primary_confidenceëŠ” ë¶„ë¥˜ì— ëŒ€í•´ ë„ˆê°€ ìƒê°í•˜ëŠ” 0.1-1.0 ë²”ìœ„ (í˜„ì‹¤ì  ìˆ˜ì¹˜)
# - JSON ë°°ì—´ë¡œë§Œ ì‘ë‹µ (ì„¤ëª… ë¶ˆí•„ìš”)
# - í…ìŠ¤íŠ¸ ìˆœì„œì™€ ê²°ê³¼ ìˆœì„œ ì¼ì¹˜ í•„ìˆ˜

# ì‘ë‹µ í˜•ì‹:
# [{"primary_category": "ì¹´í…Œê³ ë¦¬", "primary_confidence": 0.8}]"""
    
    def _create_optimized_batch_prompt(self, batch: List[str]) -> str:
        """ìµœì í™”ëœ ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # í”„ë¡¬í”„íŠ¸ í—¤ë”
        prompt = f"ë‹¤ìŒ {len(batch)}ê°œ í…ìŠ¤íŠ¸ë¥¼ ê°ê° ë¶„ë¥˜í•˜ì„¸ìš”:\n\n"
        
        # í…ìŠ¤íŠ¸ ì¶”ê°€ (ê°„ê²°í•˜ê²Œ)
        for i, chunk in enumerate(batch, 1):
            # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ëƒ„ (í† í° ì ˆì•½)
            truncated_chunk = chunk[:200] if len(chunk) > 200 else chunk
            prompt += f"{i}. {truncated_chunk}\n"
        
        # í‘¸í„°
        prompt += f"\n{len(batch)}ê°œ ê²°ê³¼ë¥¼ JSON ë°°ì—´ë¡œ ì‘ë‹µí•˜ì„¸ìš”."
        
        return prompt
    
    def _parse_optimized_batch_response(self, response_text: str, expected_count: int) -> List[CategoryResult]:
        """ìµœì í™”ëœ ë°°ì¹˜ ì‘ë‹µ íŒŒì‹±"""
        
        try:
            # JSON ì¶”ì¶œ (ë” ê´€ëŒ€í•œ íŒŒì‹±)
            json_start = response_text.find('[')
            json_end = response_text.rfind(']') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response_text[json_start:json_end]
                
                # JSON ì •ë¦¬ (ë¶ˆì™„ì „í•œ JSON ìˆ˜ì • ì‹œë„)
                json_str = self._clean_json_string(json_str)
                
                results_json = json.loads(json_str)
                
                # ê²°ê³¼ ë³€í™˜
                results = []
                for item in results_json[:expected_count]:
                    if isinstance(item, dict):
                        result = CategoryResult(
                            primary_category=self._validate_category(
                                item.get("primary_category", "career_growth")
                            ),
                            primary_confidence=self._validate_confidence(
                                item.get("primary_confidence", 0.5)
                            ),
            
                        )
                        results.append(result)
                
                return results
                
        except Exception as e:
            pass
        
        # íŒŒì‹± ì‹¤íŒ¨ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (í˜¸ì¶œìì—ì„œ í´ë°± ì²˜ë¦¬)
        return []
    
    def _clean_json_string(self, json_str: str) -> str:
        """JSON ë¬¸ìì—´ ì •ë¦¬"""
        # ë¶ˆì™„ì „í•œ JSON ìˆ˜ì • ì‹œë„
        json_str = json_str.strip()
        
        # ë§ˆì§€ë§‰ ì‰¼í‘œ ì œê±°
        json_str = re.sub(r',\s*]', ']', json_str)
        json_str = re.sub(r',\s*}', '}', json_str)
        
        # ë¶ˆì™„ì „í•œ ê°ì²´ ì™„ì„± ì‹œë„
        if json_str.endswith(','):
            json_str = json_str[:-1]
        
        return json_str
    
    def _validate_category(self, category: str) -> str:
        """ì¹´í…Œê³ ë¦¬ ìœ íš¨ì„± ê²€ì‚¬"""
        if category and category in self.categories:
            return category
        return "career_growth"  # ê¸°ë³¸ê°’
    
    def _validate_confidence(self, confidence: Any) -> float:
        """ì‹ ë¢°ë„ ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            conf = float(confidence)
            return max(0.1, min(1.0, conf))
        except (ValueError, TypeError):
            return 0.5  # ê¸°ë³¸ê°’
    
    def _create_fallback_result(self) -> CategoryResult:
        """í´ë°± ê²°ê³¼ ìƒì„±"""
        return CategoryResult(
            primary_category="career_growth",
            primary_confidence=0.3,
            secondary_category="company_culture",
            secondary_confidence=0.2
        )
    
    def get_statistics(self) -> Dict[str, Any]:
        """ë¶„ë¥˜ í†µê³„ ë°˜í™˜"""
        total_batches = self.successful_batches + self.failed_batches
        
        stats = {
            "total_api_calls": self.total_api_calls,
            "total_tokens_used": self.total_tokens_used,
            "successful_batches": self.successful_batches,
            "failed_batches": self.failed_batches,
            "success_rate": self.successful_batches / total_batches if total_batches > 0 else 0,
            "avg_tokens_per_call": self.total_tokens_used / self.total_api_calls if self.total_api_calls > 0 else 0,
            "estimated_cost_usd": self.total_tokens_used * 0.00002  # GPT-4o-mini ê°€ê²©
        }
        
        return stats

class EnhancedTextProcessor:
    """í†µí•© í…ìŠ¤íŠ¸ ì²˜ë¦¬ê¸° - ë°°ì¹˜ ìµœì í™” ë²„ì „"""
    
    def __init__(self, openai_api_key: str = None, enable_spell_check: bool = True):
        self.normalizer = TextNormalizer(enable_spell_check=enable_spell_check)
        self.classifier = OptimizedBatchClassifier(api_key=openai_api_key) if openai_api_key else None
        
        # ê°„ì†Œí™”ëœ í†µê³„
        self.stats = {
            "texts_processed": 0,
            "batches_processed": 0,
            "classification_successes": 0,
            "classification_failures": 0,
            "total_api_calls_saved": 0
        }
    
    def process_chunks_batch(self, chunks: List[str], batch_size: int = None) -> List[Dict[str, Any]]:
        """ëŒ€ìš©ëŸ‰ ì²­í¬ ë°°ì¹˜ ì²˜ë¦¬"""
        if not chunks:
            return []
        
        # ë°°ì¹˜ í¬ê¸° ì„¤ì • (Noneì´ë©´ classifierì˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
        if batch_size is None:
            batch_size = self.classifier.default_batch_size if self.classifier else 50
        
        # API í˜¸ì¶œ ì ˆì•½ ê³„ì‚° (ê°œë³„ ì²˜ë¦¬ vs ë°°ì¹˜ ì²˜ë¦¬)
        individual_calls = len(chunks)  # ê°œë³„ ì²˜ë¦¬ì‹œ í˜¸ì¶œ ìˆ˜
        batch_calls = (len(chunks) + batch_size - 1) // batch_size  # ë°°ì¹˜ ì²˜ë¦¬ì‹œ í˜¸ì¶œ ìˆ˜
        api_calls_saved = individual_calls - batch_calls
        
        print(f"ğŸ’¡ API íš¨ìœ¨ì„±:")
        print(f"   - ê°œë³„ ì²˜ë¦¬ì‹œ: {individual_calls}íšŒ í˜¸ì¶œ")
        print(f"   - ë°°ì¹˜ ì²˜ë¦¬ì‹œ: {batch_calls}íšŒ í˜¸ì¶œ")
        print(f"   - ì ˆì•½ëœ í˜¸ì¶œ: {api_calls_saved}íšŒ ({api_calls_saved/individual_calls*100:.1f}% ì ˆì•½)")
        
        self.stats["total_api_calls_saved"] = api_calls_saved
        
        # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ê·œí™” (ë¹ ë¥¸ ì²˜ë¦¬)
        print("\nğŸ“ 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ê·œí™” ì‹œì‘...")
        normalize_start = time.time()
        
        normalized_chunks = []
        for chunk in tqdm(chunks, desc="ì •ê·œí™”", unit="ì²­í¬", ncols=60):
            normalized = self.normalizer.normalize_text(chunk)
            normalized_chunks.append(normalized)
        
        normalize_time = time.time() - normalize_start
        print(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: í…ìŠ¤íŠ¸ ì •ê·œí™” ({normalize_time:.1f}ì´ˆ)")
        
        # 2ë‹¨ê³„: ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ë¶„ë¥˜
        print(f"\nğŸ“ 2ë‹¨ê³„: AI ë°°ì¹˜ ë¶„ë¥˜ ì‹œì‘... (ë°°ì¹˜í¬ê¸°: {batch_size})")
        classify_start = time.time()
        
        classification_results = []
        if self.classifier and normalized_chunks:
            try:
                classification_results = self.classifier.classify_chunks_batch(
                    normalized_chunks, batch_size=batch_size
                )
                self.stats["classification_successes"] += len(classification_results)
                self.stats["batches_processed"] += 1
                
            except Exception as e:
                print(f"\nâŒ ë°°ì¹˜ ë¶„ë¥˜ ì „ì²´ ì‹¤íŒ¨: {str(e)[:100]}...")
                self.stats["classification_failures"] += len(chunks)
                # ì „ì²´ í´ë°± ê²°ê³¼ ìƒì„±
                classification_results = [
                    CategoryResult(primary_category="career_growth", primary_confidence=0.2)
                    for _ in chunks
                ]
        
        classify_time = time.time() - classify_start
        print(f"âœ… 2ë‹¨ê³„ ì™„ë£Œ: AI ë°°ì¹˜ ë¶„ë¥˜ ({classify_time:.1f}ì´ˆ)")
        
        # 3ë‹¨ê³„: ê²°ê³¼ ì¡°í•©
        results = []
        for i, (original, normalized) in enumerate(zip(chunks, normalized_chunks)):
            classification = classification_results[i] if i < len(classification_results) else CategoryResult(primary_category="career_growth", primary_confidence=0.1)
            
            result = {
                "original_text": original,
                "normalized_text": normalized,
                "text_changed": original != normalized,
                "character_reduction": len(original) - len(normalized),
                "primary_category": classification.primary_category,
                "primary_confidence": classification.primary_confidence,
                "method": "ai_batch"
            }
            results.append(result)
        
        self.stats["texts_processed"] += len(chunks)
        
        # ìµœì¢… í†µê³„ ì¶œë ¥
        self._print_processing_summary()
        
        return results
    
    def _print_processing_summary(self):
        """ì²˜ë¦¬ ìš”ì•½ ì¶œë ¥"""
        if self.classifier:
            classifier_stats = self.classifier.get_statistics()
            
            print(f"\nğŸ“ˆ ë°°ì¹˜ ì²˜ë¦¬ ìš”ì•½:")
            print(f"   - ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸: {self.stats['texts_processed']}ê°œ")
            print(f"   - API í˜¸ì¶œ ì ˆì•½: {self.stats['total_api_calls_saved']}íšŒ")
            print(f"   - ì˜ˆìƒ ë¹„ìš© ì ˆì•½: ${self.stats['total_api_calls_saved'] * 0.002:.2f}")
            print(f"   - ì‹¤ì œ API í˜¸ì¶œ: {classifier_stats['total_api_calls']}íšŒ")
            print(f"   - ì‚¬ìš©ëœ í† í°: {classifier_stats['total_tokens_used']:,}ê°œ")
            print(f"   - ì˜ˆìƒ ì´ ë¹„ìš©: ${classifier_stats['estimated_cost_usd']:.4f}")
    
    def get_stats(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        stats = self.stats.copy()
        
        if self.classifier:
            classifier_stats = self.classifier.get_statistics()
            stats.update(classifier_stats)
        
        if stats["texts_processed"] > 0 and (stats["classification_successes"] + stats["classification_failures"]) > 0:
            success_rate = stats["classification_successes"] / (stats["classification_successes"] + stats["classification_failures"])
            stats["classification_success_rate"] = round(success_rate, 3)
        
        return stats
    
    def reset_stats(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self.stats = {
            "texts_processed": 0,
            "batches_processed": 0,
            "classification_successes": 0,
            "classification_failures": 0,
            "total_api_calls_saved": 0
        }
        
        if self.classifier:
            self.classifier.total_api_calls = 0
            self.classifier.total_tokens_used = 0
            self.classifier.successful_batches = 0
            self.classifier.failed_batches = 0

# í¸ì˜ í•¨ìˆ˜ë“¤
def normalize_text(text: str, enable_spell_check: bool = True) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™” í¸ì˜ í•¨ìˆ˜"""
    normalizer = TextNormalizer(enable_spell_check=enable_spell_check)
    return normalizer.normalize_text(text)

def classify_chunks_batch_optimized(chunks: List[str], api_key: str = None, batch_size: int = None) -> List[CategoryResult]:
    """ìµœì í™”ëœ ì²­í¬ ë°°ì¹˜ ë¶„ë¥˜ í¸ì˜ í•¨ìˆ˜"""
    classifier = OptimizedBatchClassifier(api_key=api_key)
    # batch_sizeê°€ Noneì´ë©´ classifierì˜ ê¸°ë³¸ê°’(50) ì‚¬ìš©
    return classifier.classify_chunks_batch(chunks, batch_size=batch_size)

